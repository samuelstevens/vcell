>>>> AGENTS.md
- Use `uv run SCRIPT.py` or `uv run python ARGS` to run python instead of Just plain `python`.
- After making edits, run `uvx ruff format --preview .` to format the file, then run `uvx ruff check --fix .` to lint, then run `uvx ty check FILEPATH` to type check (`ty` is prerelease software, and typechecking often will have false positives). Only do this if you think you're finished, or if you can't figure out a bug. Maybe linting will make it obvious. Don't fix linting or typing errors in files you haven't modified.
- Don't hard-wrap comments. Only use linebreaks for new paragraphs. Let the editor soft wrap content.
- Prefer negative if statements in combination with early returns/continues. Rather than nesting multiple positive if statements, just check if a condition is False, then return/continue in a loop. This reduces indentation.
- This project uses Python 3.12. You can use `dict`, `list`, `tuple` instead of the imports from `typing`. You can use `| None` instead of `Optional`.
- Use single-backticks for variables. We use Markdown and [pdoc3](https://pdoc3.github.io/pdoc/) for docs rather than ReST and Sphinx.
- File descriptors from `open()` are called `fd`.
- Use types where possible, including `jaxtyping` hints.
- Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
- Variables referring to a absolute filepath should be suffixed with `_fpath`. Filenames are `_fname`. Directories are `_dpath`.
- Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
- Only use `setup` for naming functions that don't return anything.

- You can use `gh` to access issues and PRs on GitHub to gather more context. We use GitHub issues a lot to share ideas and communicate about problems, so you should almost always check to see if there's a relevant GitHub issue for whatever you're working on.

# Tensor Variables

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).
The key for these suffixes:

- b: batch size
- d: model dimension

For example, an batched activation tensor with shape (batch, d_model) is `x_bd`.

>>>> CLAUDE.md
- Use `uv run SCRIPT.py` or `uv run python ARGS` to run python instead of Just plain `python`.
- After making edits, run `uvx ruff format --preview .` to format the file, then run `uvx ruff check --fix .` to lint, then run `uvx ty check FILEPATH` to type check (`ty` is prerelease software, and typechecking often will have false positives). Only do this if you think you're finished, or if you can't figure out a bug. Maybe linting will make it obvious. Don't fix linting or typing errors in files you haven't modified.
- Don't hard-wrap comments. Only use linebreaks for new paragraphs. Let the editor soft wrap content.
- Prefer negative if statements in combination with early returns/continues. Rather than nesting multiple positive if statements, just check if a condition is False, then return/continue in a loop. This reduces indentation.
- This project uses Python 3.12. You can use `dict`, `list`, `tuple` instead of the imports from `typing`. You can use `| None` instead of `Optional`.
- Use single-backticks for variables. We use Markdown and [pdoc3](https://pdoc3.github.io/pdoc/) for docs rather than ReST and Sphinx.
- File descriptors from `open()` are called `fd`.
- Use types where possible, including `jaxtyping` hints.
- Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
- Variables referring to a absolute filepath should be suffixed with `_fpath`. Filenames are `_fname`. Directories are `_dpath`.
- Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
- Only use `setup` for naming functions that don't return anything.

- You can use `gh` to access issues and PRs on GitHub to gather more context. We use GitHub issues a lot to share ideas and communicate about problems, so you should almost always check to see if there's a relevant GitHub issue for whatever you're working on.

# Tensor Variables

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).
The key for these suffixes:

- b: batch size
- d: model dimension

For example, an batched activation tensor with shape (batch, d_model) is `x_bd`.

>>>> CONTRIBUTING.md
# Contributing

## TL;DR

Install [uv](https://docs.astral.sh/uv/).
Clone this repository, then from the root directory:

```sh
uv run python  # TODO
```

You also need [yek](https://github.com/bodo-run/yek) for generating docs.

## Coding Style & Conventions

* Don't hard-wrap comments. Only use linebreaks for new paragraphs. Let the editor soft wrap content.
* Use single-backticks for variables. We use Markdown and [pdoc3](https://pdoc3.github.io/pdoc/) for docs rather than ReST and Sphinx.
* File descriptors from `open()` are called `fd`.
* Use types where possible, including `jaxtyping` hints.
* Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
* Variables referring to a filepath should be suffixed with `_fpath`. Directories are `_dpath`.
* Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
* Only use `setup` for naming functions that don't return anything.

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).

The key for these suffixes:

* b: batch size
* d: model dimension

For example, an batched activation tensor with shape (batch, d_model) is `x_bd`.

## Testing & Linting

`justfile` contains commands for testing and linting.

`just lint` will format and lint.
`just test` will format, lint and test, then report coverage.

## Commit / PR Checklist

1. Run `just test`.
2. Check that there are no regressions. Unless you are certain tests are not needed, the coverage % should either stay the same or increase.
3. Run `just docs`.
4. Fix any missing doc links.

>>>> README.md
# vcell

[Shared Google Doc](https://docs.google.com/document/d/1oHJQUAFk9mrhPrLqkglq_e3yufYDEOKCF3VvDT-MHpc/edit?usp=sharing)

This is the repo for a [virtual cell challenge](https://virtualcellchallenge.org) submission.

To install, clone this repository (via SSH is probably easier, but you do you).

In the project root directory, run `uv run experiments/00_mean_baseline.py`.
The first invocation should create a virtual environment.

## Getting Started

1. (If you have a Google Cloud project and approved TPU usage) Install `gcloud`. The [official instructions](https://cloud.google.com/sdk/docs/install) are shitty. [Here's what I did](src/vcell/install-gcloud.md).
2. [Make a TPU VM](src/vcell/make-a-tpu-vm.md).
3. Clone this repo onto your TPU VM.
4. [Sync the input data to the TPU VM](src/vcell/tpu-tricks.md).
5. Run training.
6. [Sync the outputs to your local machine](src/vcell/tpu-tricks.md).
7. Run `uv run scripts/submit_vcc.py --pred pred_raw.h5ad`.
8. Submit `pred_raw.prep.vcc` to the leaderboard.

## Setup

**Editor:**
I would add `docs/api/` to whatever `.ignore`-like system you have so that they don't show up.
We commit them to git so that they can be browsed on GitHub, but you don't want to edit them by hand because they are generated by [pdoc3](https://pdoc3.github.io/pdoc/).

**Dependencies:** [uv](https://docs.astral.sh/uv/) is the new dependency manager in Python.
There's a short blog post [here](https://samuelstevens.me/writing/uv) describing uv but it's well documented all over the internet.
We use it in this project for the easiest dependency management ever.
Please install it--it's very high quality software.

**Automated Tooling:** [just](https://github.com/casey/just) is a decent language-agnostic replacement for `package.json` scripts.
It supports dependencies and is pretty fast.
It can be installed with brew (`brew install just`) or from a binary because it doesn't really need to be updated frequently.

> I am happy to change to make or another language-agnostic system if we don't want to go with just.

If you run `just --list` in any folder in the repo, it will list all the recipes.
Running `just` without any args will run the first recipe and its dependencies.

When writing new recipes, assume that you are on a generic unix-like system and that the only non-unix tools are `uv` and `just` itself.
So don't use `fd`, `rg`, `npm`, etc.

**AI Tooling (Optional):**
My favorite is [aider](https://aider.chat/).
I won't put a whole tutorial here, but I use Sonnet-3.5 with $20 of API credits loaded up.
The [AGENTS.md](AGENTS.md) file is used by aider so that the coding assistant follows conventions by default.
I symlink CLAUDE.md to AGENTS.md so that Claude Code will read it.

## Data

You need to make an account on [https://virtualcellchallenge.org](https://virtualcellchallenge.org) and then you can download the data.
The data provided by the challenge is 15GB.

Data should be extracted to the root project folder. Final structure will look like: `/vcell/data/vcc_data`.

## Docs

API docs are generated from Python docstrings using [pdoc3](https://pdoc3.github.io/pdoc/).
You can run `just docs` to generate new API docs in `docs/api`.
To write good docstrings that can be parsed, try to follow the conventions in existing code.

The docs recipe also generates `docs/api/llms.txt` which is all of the API docs in one .txt file.
This is great for copy-pasting into an LLM to ask it how to do something with our codebase.

## Dependencies

I hate dependencies.
I haaaaate dependencies.
That being said, here are some great tools that I use in nearly every project (including this one).

[tyro](https://brentyi.github.io/tyro/) turns functions into Python scripts.
By adding docstrings and types to everything (good practice anyways!) you can get great command-line interfaces to your Python scripts.

[beartype](https://beartype.readthedocs.io/en/stable/) is an "open-source pure-Python PEP-compliant near-real-time hybrid runtime-static third-generation type-checker."
While the docs can be a little corny, it adds very fast runtime type checking to your functions, meaning you never need to write any code that checks for types.
It is a runtime alternative to MyPy for when you have some objects typed `any` that you still want to type-check.
Please decorate every function with it.

>>>> __init__.py
"""
.. include:: install-gcloud.md

.. include:: make-a-tpu-vm.md

.. include:: tpu-tricks.md
"""

>>>> data/__init__.py
from .naive_dataloader import Config as PerturbationConfig
from .naive_dataloader import PerturbationDataloader

__all__ = ["PerturbationConfig", "PerturbationDataloader"]

>>>> data/harmonize.py
import collections
import dataclasses
import pathlib
import re
import typing as tp

import anndata as ad
import beartype
import numpy as np
import scanpy as sc
from jaxtyping import Bool, Int, jaxtyped


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class GeneMap:
    """Mapping from a dataset's columns to the canonical VCC gene space."""

    n_genes: int
    """Number of VCC genes"""
    present_mask: Bool[np.ndarray, " G"]
    """which VCC genes exist in this dataset"""
    ds_cols: Int[np.ndarray, " K"]
    """dataset column indices to take"""
    vcc_cols: Int[np.ndarray, " K"]
    """destination VCC columns"""
    stats: dict[str, int]
    """counts for sanity reporting"""

    def lift_to_vcc(self, x_ds: Int[np.ndarray, "..."]) -> Int[np.ndarray, "..."]:
        """
        Project dataset matrix slice (n, n_vars_ds) into VCC order (n, G), filling missing with zeros.
        """
        out = np.zeros((x_ds.shape[0], self.n_genes), dtype=np.float32)
        out[:, self.vcc_cols] = x_ds[:, self.ds_cols]
        return out


@beartype.beartype
class GeneVocab:
    """
    Canonical VCC gene space built from the VCC .h5ad.
    - Prefers Ensembl IDs (stable).
    - Keeps symbols for unique-only fallback.
    """

    def __init__(self, vcc_h5ad: str):
        vcc = sc.read(vcc_h5ad, backed="r")
        if "gene_id" not in vcc.var.columns:
            raise ValueError(
                "Expected VCC .var to contain a 'gene_id' column (Ensembl)."
            )

        self.n_genes = vcc.n_vars

        self.vcc_ens: list[str] = [_strip_ens_version(s) for s in vcc.var["gene_id"]]
        self.vcc_sym: list[str] = vcc.var.index.astype(str).tolist()

        # Ensembl -> VCC index (unique by construction)
        self._ens_to_idx: dict[str, int] = {e: i for i, e in enumerate(self.vcc_ens)}
        # Symbol -> list of indices (can be non-unique)
        self._sym_to_idxs: dict[str, list[int]] = collections.defaultdict(list)
        for i, s in enumerate(self.vcc_sym):
            self._sym_to_idxs[s].append(i)

    def make_map(
        self, ds: ad.AnnData, dup_mode: tp.Literal["sum", "keep", None] = None
    ) -> GeneMap:
        """
        Create a GeneMap from a dataset.
        """

        if dup_mode is None:
            # Try to figure out whether we have raw counts (integers) or log-normalized counts (floats, smaller).
            row = ds.X[0]
            row = row.toarray() if hasattr(row, "toarray") else np.asarray(row)
            if row.max() > 100:
                # Probably raw counts
                dup_mode = "sum"
            elif row[row > 1].min() < 2.0:
                dup_mode = "keep"
            else:
                if ds.isbacked:
                    self.logger.warning(
                        "Not sure whether ds '%s' is raw counts or log normalized.",
                        ds.filename,
                    )
                else:
                    self.logger.warning(
                        "Not sure whether ds is raw counts or log normalized."
                    )

        ds_sym = list(ds.var_names)
        ds_ens = [_strip_ens_version(s) for s in ds.var["ensembl_id"].tolist()]

        assert len(ds_sym) == len(ds_ens)

        present_mask = np.zeros(self.n_genes, dtype=bool)
        ds_cols: list[int] = []
        vcc_cols: list[int] = []

        n_ens_match = 0
        n_sym_match = 0
        n_sym_ambig = 0

        for j, (ens, sym) in enumerate(zip(ds_ens, ds_sym)):
            if ens and ens in self._ens_to_idx:
                i = self._ens_to_idx[ens]
                ds_cols.append(j)
                vcc_cols.append(i)
                present_mask[i] = True
                n_ens_match += 1
            else:
                cand = self._sym_to_idxs.get(sym, [])
                if len(cand) == 1:
                    i = cand[0]
                    ds_cols.append(j)
                    vcc_cols.append(i)
                    present_mask[i] = True
                    n_sym_match += 1
                elif len(cand) > 1:
                    n_sym_ambig += 1
                    # skip ambiguous symbols

        ds_cols = np.asarray(ds_cols, dtype=int)
        vcc_cols = np.asarray(vcc_cols, dtype=int)
        stats = dict(
            vcc_genes=self.n_genes,
            ds_vars=len(ds_sym),
            matched_by_ensembl=int(n_ens_match),
            matched_by_symbol=int(n_sym_match),
            skipped_ambiguous_symbol=int(n_sym_ambig),
            total_matched=int(len(ds_cols)),
            coverage=int(present_mask.sum()),
        )
        return GeneMap(
            n_genes=self.n_genes,
            present_mask=present_mask,
            ds_cols=ds_cols,
            vcc_cols=vcc_cols,
            stats=stats,
        )


@beartype.beartype
def _strip_ens_version(s: str) -> str:
    """ENSG00000187634.5 -> ENSG00000187634"""
    return re.sub(r"\.\d+$", "", s)


@beartype.beartype
class Dataloader:
    def __init__(self, vcc_a5hd_path: pathlib.Path, scperturb_a5hd_path: pathlib.Path):
        self.gene_vocab = GeneVocab(vcc_a5hd_path)
        self.sc_adata = sc.read(scperturb_a5hd_path, backed="r")
        self.gene_map = self.gene_vocab.make_map(self.sc_adata)

        unique_perts = self.sc_adata.obs[self.pert_col].unique()
        self.pert2id = {pert: i for i, pert in enumerate(sorted(unique_perts))}

    def __iter__(self) -> collections.abc.Iterator[dict]:
        pool = []
        # Randomly sample from the pool
        while True:
            if not pool:
                # If pool is empty, regenerate it
                pool = self.adata.obs.get_columns(self.pert_col).to_list()

            # Randomly select a combination
            idx = self.rng.integers(0, len(pool))
            pert = pool.pop(idx)

            # Get perturbed cells for this combination
            pert_mask = self.adata.obs[self.cfg.pert_col] == pert
            pert_indices = np.where(pert_mask)[0]

            # Skip if not enough perturbed cells
            if len(pert_indices) < self.cfg.set_size:
                self.logger.debug(
                    "Skipping (%s, %s) with only %d indices.",
                    cell_line,
                    pert,
                    len(pert_indices),
                )
                continue

            # Just match cell line for controls
            control_mask = (self.adata.obs[self.cfg.cell_line_col] == cell_line) & (
                self.adata.obs[self.cfg.pert_col] == self.cfg.control_code
            )
            control_indices = np.where(control_mask)[0]

            # Skip if no control cells available
            if len(control_indices) == 0:
                self.logger.debug(
                    "Skipping (%s, %s) (no control cells).", cell_line, pert
                )
                continue

            # Sample perturbed cells
            sampled_pert_indices = self.rng.choice(
                pert_indices, size=self.cfg.set_size, replace=False
            )

            # Sample control cells (with replacement if needed)
            sampled_control_indices = self.rng.choice(
                control_indices, size=self.cfg.set_size, replace=True
            )

            # Get the expression data
            control_data = self._get_cell_data(sampled_control_indices.tolist())
            target_data = self._get_cell_data(sampled_pert_indices.tolist())

            yield {
                "control": control_data,
                "target": target_data,
                "pert": self.pert2id[pert],
                "cell_line": self.cline2id[cell_line],
            }

>>>> data/naive_dataloader.py
# src/vcell/data/naive_dataloader.py
import collections
import dataclasses
import difflib
import logging
import pathlib
import warnings

import beartype
import numpy as np
import scanpy as sc


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    h5ad_fpath: pathlib.Path
    """Path to the h5ad file containing perturbation data."""

    set_size: int = 256
    """Number of cells to include in each example set."""

    genes: list[str] = dataclasses.field(default_factory=list)
    """List of gene names to select from the dataset. If empty, all genes are used."""

    pert_col: str = "target_gene"
    """Column name in the AnnData object that contains perturbation information."""

    cell_line_col: str = "cell_line"
    """Column name in the AnnData object that contains cell line information."""

    control_code: str = "non-targeting"
    """Value that identifies control cells in the perturbation column."""

    seed: int = 0
    """Random seed."""


@beartype.beartype
class PerturbationDataloader:
    """
    At each step, randomly samples one unique combination of cell line and perturbation. If there are no observations for the combination, it samples another.
    """

    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.logger = logging.getLogger(self.__class__.__name__)
        self.rng = np.random.default_rng(seed=cfg.seed)

        # Read the h5ad file
        self.adata = sc.read(self.cfg.h5ad_fpath, backed="r")

        # Apply genes if provided
        if self.cfg.genes:
            # Try to find matching genes in the data
            available_genes = self.adata.var_names.tolist()
            selected_indices = []
            for gene in self.cfg.genes:
                if gene in available_genes:
                    selected_indices.append(available_genes.index(gene))
            if selected_indices:
                self.adata = self.adata[:, selected_indices]

        # Check if data needs normalization
        self._ensure_normalized()

        # Validate column names exist
        self._validate_column(self.cfg.pert_col)
        self._validate_column(self.cfg.cell_line_col)

        # Create ID mappings first
        unique_perts = self.adata.obs[self.cfg.pert_col].unique()
        self.pert2id = {pert: i for i, pert in enumerate(sorted(unique_perts))}

        unique_clines = self.adata.obs[self.cfg.cell_line_col].unique()
        self.cline2id = {cline: i for i, cline in enumerate(sorted(unique_clines))}

    def __iter__(self) -> collections.abc.Iterator[dict]:
        pool = []
        # Randomly sample from the pool
        while True:
            if not pool:
                # If pool is empty, regenerate it
                pool = list(
                    self.adata.obs.groupby(
                        [self.cfg.cell_line_col, self.cfg.pert_col], observed=False
                    ).groups.keys()
                )

            # Randomly select a combination
            idx = self.rng.integers(0, len(pool))
            cell_line, pert = pool.pop(idx)

            # Get perturbed cells for this combination
            pert_mask = (self.adata.obs[self.cfg.cell_line_col] == cell_line) & (
                self.adata.obs[self.cfg.pert_col] == pert
            )
            pert_indices = np.where(pert_mask)[0]

            # Skip if not enough perturbed cells
            if len(pert_indices) < self.cfg.set_size:
                self.logger.debug(
                    "Skipping (%s, %s) with only %d indices.",
                    cell_line,
                    pert,
                    len(pert_indices),
                )
                continue

            # Just match cell line for controls
            control_mask = (self.adata.obs[self.cfg.cell_line_col] == cell_line) & (
                self.adata.obs[self.cfg.pert_col] == self.cfg.control_code
            )
            control_indices = np.where(control_mask)[0]

            # Skip if no control cells available
            if len(control_indices) == 0:
                self.logger.debug(
                    "Skipping (%s, %s) (no control cells).", cell_line, pert
                )
                continue

            # Sample perturbed cells
            sampled_pert_indices = self.rng.choice(
                pert_indices, size=self.cfg.set_size, replace=False
            )

            # Sample control cells (with replacement if needed)
            sampled_control_indices = self.rng.choice(
                control_indices, size=self.cfg.set_size, replace=True
            )

            # Get the expression data
            control_data = self._get_cell_data(sampled_control_indices.tolist())
            target_data = self._get_cell_data(sampled_pert_indices.tolist())

            yield {
                "control": control_data,
                "target": target_data,
                "pert": self.pert2id[pert],
                "cell_line": self.cline2id[cell_line],
            }

    def _ensure_normalized(self):
        """Ensure data is depth-normalized and log1p transformed."""
        # For backed mode, we assume data is already normalized
        # This is because we can't modify backed data
        # In production, this should be checked/documented

        self.logger.warning("Data normalization check not implemented for backed mode")
        warnings.warn(
            "Normalization check not implemented for backed AnnData objects",
            UserWarning,
        )

    def _validate_column(self, col_name: str) -> None:
        """Validate that a column exists, suggesting alternatives if not."""
        if col_name not in self.adata.obs.columns:
            available_cols = list(self.adata.obs.columns)
            # Find similar column names
            suggestions = difflib.get_close_matches(
                col_name, available_cols, n=5, cutoff=0.4
            )

            # Combine suggestions
            all_suggestions = list(dict.fromkeys(suggestions))[:5]

            error_msg = f"Column '{col_name}' not found in AnnData.obs."
            if all_suggestions:
                error_msg += f" Did you mean one of these? {all_suggestions}"
            error_msg += f" Available columns: {available_cols[:10]}{'...' if len(available_cols) > 10 else ''}"

            raise KeyError(error_msg)

    def _get_cell_data(self, indices: list[int]) -> np.ndarray:
        """Get expression data for given cell indices."""
        # h5py requires indices to be sorted and unique
        # Create mapping to handle duplicates
        unique_indices = sorted(set(indices))

        # Get data for unique indices
        data = self.adata[unique_indices].X
        if hasattr(data, "toarray"):
            data = data.toarray()

        # Create lookup for unique indices
        idx_to_data = {idx: data[i] for i, idx in enumerate(unique_indices)}

        # Build result with proper duplicates
        result = np.stack([idx_to_data[idx] for idx in indices])

        return result.astype(np.float32)

>>>> helpers.py
# src/vcell/helpers.py
import collections.abc
import logging
import pathlib
import subprocess
import time

import beartype


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress", total: int = 0):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
            total: If non-zero, how long the iterable is.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)
        self.total = total

    def __iter__(self):
        start = time.time()

        try:
            total = len(self)
        except TypeError:
            total = None

        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if total is not None:
                    pred_min = (total - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        total,
                        (i + 1) / total * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        if self.total > 0:
            return self.total

        # Will throw exception.
        return len(self.it)


@beartype.beartype
class batched_idx:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """

    def __init__(self, total_size: int, batch_size: int):
        """
        Args:
            total_size: total number of examples
            batch_size: maximum distance between the generated indices
        """
        self.total_size = total_size
        self.batch_size = batch_size

    def __iter__(self) -> collections.abc.Iterator[tuple[int, int]]:
        """Yield (start, end) index pairs for batching."""
        for start in range(0, self.total_size, self.batch_size):
            stop = min(start + self.batch_size, self.total_size)
            yield start, stop

    def __len__(self) -> int:
        """Return the number of batches."""
        return (self.total_size + self.batch_size - 1) // self.batch_size


@beartype.beartype
def current_git_commit() -> str | None:
    """
    Best-effort short SHA of the repo containing *this* file.

    Returns `None` when
    * `git` executable is missing,
    * we’re not inside a git repo (e.g. installed wheel),
    * or any git call errors out.
    """
    try:
        # Walk up until we either hit a .git dir or the FS root
        here = pathlib.Path(__file__).resolve()
        for parent in (here, *here.parents):
            if (parent / ".git").exists():
                break
        else:  # no .git found
            return None

        result = subprocess.run(
            ["git", "-C", str(parent), "rev-parse", "--short", "HEAD"],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            check=True,
        )
        return result.stdout.strip() or None
    except (FileNotFoundError, subprocess.CalledProcessError):
        return None

>>>> install-gcloud.md
## Install `gcloud`

A simple guide to installing the `gcloud` binary so that it can be easily uninstalled.

1. Download and extract the package.
2. Move it somewhere useful.
3. Add the directory to your $PATH.
4. Login to Google Cloud.

Download the right package from https://cloud.google.com/sdk/docs/install for your computer.

<details>
<summary>My Choice</summary>

I clicked the macOS tab and then chose the macOS 64-bit Apple Silicon option: https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz

You can download and extract this to whatever directory you want. We will move it.

</details>

```
$ pwd
/Users/samstevens/Development/vcell
$ wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz
--2025-08-15 09:51:22--  https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz
Resolving dl.google.com (dl.google.com)... 74.125.21.91, 74.125.21.190, 74.125.21.136, ...
Connecting to dl.google.com (dl.google.com)|74.125.21.91|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 56538937 (54M) [application/gzip]
Saving to: ‘google-cloud-cli-darwin-arm.tar.gz’

google-cloud-cli-darwin-arm.tar.gz              100%[====================================================================================================>]  53.92M  31.4MB/s    in 1.7s

2025-08-15 09:51:24 (31.4 MB/s) - ‘google-cloud-cli-darwin-arm.tar.gz’ saved [56538937/56538937]
$ ls
google-cloud-cli-darwin-arm.tar.gz
$ tar -xzf google-cloud-cli-darwin-arm.tar.gz  # -x extract -z use gzip -f filepath
$ ls
google-cloud-cli-darwin-arm.tar.gz  google-cloud-sdk/
```

I store all these crappy non-pip tools in `~/.local/pkg`. You can put it wherever you want because we will eventually add it to our `$PATH`.

```
$ mv google-cloud-sdk ~/.local/pkg
$ ls ~/.local/pkg
aws-cli/  google-cloud-sdk/
```

Then I add the `bin/` to my path.
I use fish, so it's just `fish_add_path ~/.local/pkg/google-cloud-sdk/bin`.
If you use a different shell, then do whatever you do to add `~/.local/pkg/google-cloud-sdk/bin` to your path.

Then you can run:

```
$ gcloud version
Google Cloud SDK 533.0.0
bq 2.1.22
core 2025.08.01
gcloud-crc32c 1.0.0
gsutil 5.35
Updates are available for some Google Cloud CLI components.  To install them,
please run:
  $ gcloud components update
```

Now you have gcloud installed in a way that is easy to delete!
You need to run `glcoud init` and (maybe) `gcloud auth login` and login with your google account.

>>>> make-a-tpu-vm.md
## Make a TPU VM

TL;DR:

```sh
gcloud compute tpus tpu-vm create $NAME \
  --project $PROJECT_ID \
  --zone $ZONE \
  --accelerator-type $TPU_VERSION \
  --version $IMAGE
```

* `$NAME` can be whatever you want. You will reference it when you want to connect to the TPU VM.
* `$PROJECT_ID` is the project ID you shared with the TPU Research Cloud program folks. It's in the email you get from them (see below).
* The `$TPU_VERSION` and `$ZONE` variables depend on what you get access to. That's in your email.
* The `$IMAGE` variable depends on the `$TPU_VERSION`, and you have to look it up here: https://cloud.google.com/tpu/docs/runtimes. v4 and older use tpu-ubuntu2204-base.

![Image of my email from TRC, showing that I have access to `v2-8` TPU VMs in `us-central1-f`](/docs/assets/tpu-email.jpg)

You will need to run this command multiple times until it succeeds.

Once you have a TPU VM, you can see them all with:

```
$ gcloud compute tpus tpu-vm list --project $PROJECT_ID --zone $ZONE
NAME   ZONE           ACCELERATOR_TYPE  TYPE  TOPOLOGY  NETWORK  RANGE          STATUS
tpu-2  us-central1-f  v2-8              V2    2x2       default  10.128.0.0/20  READY
```

Then you can connect via SSH:

```sh
gcloud compute tpus tpu-vm ssh $NAME \
  --project $PROJECT_ID \
  --zone $ZONE
```

You should have sudo access.

>>>> metrics.py
# src/vcell/metrics.py
import typing as tp

import beartype
import equinox as eqx
import jax.numpy as jnp
from jaxtyping import Array, Float, jaxtyped

__all__ = ["DEDetails", "RunningMean", "compute_mae", "compute_pds", "compute_de"]


@jaxtyped(typechecker=beartype.beartype)
class DEDetails(eqx.Module):
    """Immutable result packet for DE agreement metrics."""

    overlap: float
    """e.g., Jaccard/F1 over true-significant genes."""
    pr_auc: float
    """PR-AUC of predicted DE vs true DE."""
    spearman_r: float
    """rank corr of (signed) logFC."""
    n_true_sig: int
    n_pred_sig: int


@jaxtyped(typechecker=beartype.beartype)
class RunningMean(eqx.Module):
    """Numerically stable running mean of scalar values.

    Note: prefer returning a *new* instance rather than in-place mutation
    to keep things functional/JAX-friendly.
    """

    total: Float[Array, ""]
    count: Float[Array, ""]

    @staticmethod
    @jaxtyped(typechecker=beartype.beartype)
    def zero() -> "RunningMean":
        return RunningMean(total=jnp.array(0.0), count=jnp.array(0.0))

    @jaxtyped(typechecker=beartype.beartype)
    def update(
        self, value: Float[Array, ""], weight: Float[Array, ""] | int = 1
    ) -> "RunningMean":
        total = self.total + value * jnp.asarray(weight, dtype=self.total.dtype)
        count = self.count + jnp.asarray(weight, dtype=self.count.dtype)
        return RunningMean(total=total, count=count)

    @jaxtyped(typechecker=beartype.beartype)
    def merge(self, other: "RunningMean") -> "RunningMean":
        return RunningMean(
            total=self.total + other.total, count=self.count + other.count
        )

    @jaxtyped(typechecker=beartype.beartype)
    def compute(self) -> Float[Array, ""]:
        # Returns NaN if count==0; caller should guard or accept NaN to signal "empty".
        return self.total / self.count


@jaxtyped(typechecker=beartype.beartype)
def compute_mae(
    pred: Float[Array, "... g"],
    true: Float[Array, "... g"],
    *,
    mask: Float[Array, "... g"] | None = None,
) -> Float[Array, "..."]:
    """Per-example MAE across genes. Reduces over the last (gene) axis only.

    Shapes:
      pred, true: [..., g]
      mask (optional): same shape; 1.0 keeps a gene, 0.0 drops it.
    Returns:
      mae: [...]  (one scalar per leading example/perturbation index)
    """
    diff = jnp.abs(pred - true)
    if mask is not None:
        # Avoid divide-by-zero: normalise by sum(mask) along gene axis.
        masked = diff * mask
        denom = jnp.clip(jnp.sum(mask, axis=-1), a_min=1e-12)
        return jnp.sum(masked, axis=-1) / denom
    return jnp.mean(diff, axis=-1)


@jaxtyped(typechecker=beartype.beartype)
def compute_pds(
    pred: Float[Array, "p g"],
    true: Float[Array, "p g"],
    *,
    topk: tuple[int, ...] = (1, 5, 10),
) -> dict[str, Float[Array, ""]]:
    """Perturbation Discrimination Score (skeleton).

    Intent:
      For each perturbation i, rank true profiles by distance to pred[i].
      Report mean inverse rank and top-k accuracy.

    Returns:
        {
            "mean_inv_rank": float,
            "top1": float,
            "top5": float,
            ...
        }
    """
    p, g = pred.shape
    if p == 0:
        raise ValueError("p (number of perturbations) must be > 0.")

    # Pairwise L1 distances, smaller is better.
    # [p, p, g] -> [p, p]
    dist = jnp.abs(pred[:, None, :] - true[None, :, :]).sum(-1)

    # Regular ranking logic
    order = jnp.argsort(dist, axis=1, stable=True)
    true_idx = jnp.arange(p)
    # Rank = first position (1-based) where the correct index appears
    pos = jnp.argmax(order == true_idx[:, None], axis=1) + 1
    mrr = jnp.mean(1.0 / pos.astype(jnp.float32))

    # Top-k metrics (dedup + clamp)
    uniq_topk = tuple(sorted(set(topk)))
    out = {"mean_inv_rank": mrr}
    for k in uniq_topk:
        kk = int(max(1, min(k, p)))
        top = jnp.mean((pos <= kk).astype(jnp.float32))
        out[f"top{kk}"] = top
    return out


@jaxtyped(typechecker=beartype.beartype)
def compute_de(
    pred_pert: Float[Array, "r g"],
    pred_ctrl: Float[Array, "r g"],
    true_pert: Float[Array, "r g"],
    true_ctrl: Float[Array, "r g"],
    *,
    fdr: float = 0.05,
    test: tp.Literal["wilcoxon"] = "wilcoxon",
    two_sided: bool = True,
) -> DEDetails:
    """Differential-expression agreement (skeleton).

    Intent:
      1) Make DE calls (pred vs ctrl) and (true vs ctrl) with the same test+FDR.
      2) Compare sets/ranks: overlap (e.g., Jaccard/F1), PR-AUC, Spearman on |logFC| or signed logFC.

    Notes:
      - Callers should pass consistent normalisation/gene order.
      - r = number of 'replicates' per condition (can be 1 if using pseudobulks).

    Returns:
      DEDetails(overlap=..., pr_auc=..., spearman_r=..., n_true_sig=..., n_pred_sig=...)
    """
    raise NotImplementedError(
        "compute_de is a skeleton; implement DE test, BH-FDR, and comparisons."
    )

>>>> nn/loss.py
import beartype
import jax.numpy as jnp
from jaxtyping import Array, Float, jaxtyped


@jaxtyped(typechecker=beartype.beartype)
def _pairwise_lp(X, Y, p: float = 1.0, eps: float = 1e-12):
    """Return pairwise ||x_i - y_j||^p. Shapes: X [S,G], Y [S,G] -> [S,S]."""
    # squared euclidean
    x2 = (X * X).sum(-1, keepdims=True)  # [S,1]
    y2 = (Y * Y).sum(-1, keepdims=True).T  # [1,S]
    d2 = jnp.clip(x2 + y2 - 2.0 * (X @ Y.T), 0.0, jnp.inf)  # numerical safety
    if p == 2.0:
        return d2  # already squared L2
    # general p: ||.||^p = (||.||_2^2)^(p/2)
    return jnp.power(d2 + eps, 0.5 * p)


@jaxtyped(typechecker=beartype.beartype)
def mmd2_energy_kernel(
    pred: Float[Array, "set genes"], target: Float[Array, "set genes"], p: float = 1.0
) -> Float[Array, ""]:
    """
    The squared MMD between the predicted and observed cell sets is computed in Eq. (19) of the STATE paper.
    """
    s, g = pred.shape
    d_pp = _pairwise_lp(pred, pred, p)
    d_tt = _pairwise_lp(target, target, p)
    d_pt = _pairwise_lp(pred, target, p)
    # k = -dist^p
    k_pp = -d_pp
    k_tt = -d_tt
    k_pt = -d_pt
    mmd2 = (k_pp.sum() + k_tt.sum() - 2.0 * k_pt.sum()) / (s * s)
    return mmd2

>>>> tpu-tricks.md
## TPU Tricks

`gcloud` can `scp` files and folders to and from TPU VMs.

For instance, to get the vcc.zip file onto your TPU VM:

```sh
gcloud compute tpus tpu-vm scp $VCC_ZIP $NAME:$PROJECT_ROOT/data/inputs \
  --project $PROJECT_ID \
  --zone $ZONE
```

For me, from my project root, I run:

```sh
gcloud compute tpus tpu-vm scp data/inputs/vcc.zip tpu-2:projects/vcell/data/inputs --project trc-project-466816 --zone us-central1-f
```

Then on my TPU VM, I just run `unzip vcc.zip`.

Or to get the predictions file onto my laptop so that I can submit it:

```sh
gcloud compute tpus tpu-vm scp $NAME:$PROJECT_ROOT/pred_raw.h5ad . \
  --project $PROJECT_ID \
  --zone $ZONE
```

For me, from my project root, I run:

```sh
gcloud compute tpus tpu-vm scp tpu-2:~/projects/vcell/pred_raw.h5ad . --project trc-project-466816 --zone us-central1-f
```

Note the `.` to signify where on your laptop it should go; `.` means this directory.

## Adding Persistent Disks

gcloud compute disks create tpu-data --size 200GB --type pd-standard --zone us-central1-f --project trc-project-466816

gcloud alpha compute tpus tpu-vm attach-disk tpu-2 --zone us-central1-f --disk tpu-data --project trc-project-466816 --mode read-write


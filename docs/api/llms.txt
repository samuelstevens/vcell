>>>> AGENTS.md
- Use `uv run SCRIPT.py` or `uv run python ARGS` to run python instead of Just plain `python`.
- After making edits, run `uvx ruff format --preview .` to format the file, then run `uvx ruff check --fix .` to lint, then run `uvx ty check FILEPATH` to type check (`ty` is prerelease software, and typechecking often will have false positives). Only do this if you think you're finished, or if you can't figure out a bug. Maybe linting will make it obvious. Don't fix linting or typing errors in files you haven't modified.
- Don't hard-wrap comments. Only use linebreaks for new paragraphs. Let the editor soft wrap content.
- Prefer negative if statements in combination with early returns/continues. Rather than nesting multiple positive if statements, just check if a condition is False, then return/continue in a loop. This reduces indentation.
- This project uses Python 3.12. You can use `dict`, `list`, `tuple` instead of the imports from `typing`. You can use `| None` instead of `Optional`.
- Use single-backticks for variables. We use Markdown and [pdoc3](https://pdoc3.github.io/pdoc/) for docs rather than ReST and Sphinx.
- File descriptors from `open()` are called `fd`.
- Use types where possible, including `jaxtyping` hints.
- Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
- Variables referring to a absolute filepath should be suffixed with `_fpath`. Filenames are `_fname`. Directories are `_dpath`.
- Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
- Only use `setup` for naming functions that don't return anything.

- You can use `gh` to access issues and PRs on GitHub to gather more context. We use GitHub issues a lot to share ideas and communicate about problems, so you should almost always check to see if there's a relevant GitHub issue for whatever you're working on.
- Write single-line commit messages; never say you co-authored a commit.

# Tensor Variables

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).
The key for these suffixes:

- b: batch size
- d: model dimension

For example, an batched activation tensor with shape (batch, d_model) is `x_bd`.

>>>> CLAUDE.md
- Use `uv run SCRIPT.py` or `uv run python ARGS` to run python instead of Just plain `python`.
- After making edits, run `uvx ruff format --preview .` to format the file, then run `uvx ruff check --fix .` to lint, then run `uvx ty check FILEPATH` to type check (`ty` is prerelease software, and typechecking often will have false positives). Only do this if you think you're finished, or if you can't figure out a bug. Maybe linting will make it obvious. Don't fix linting or typing errors in files you haven't modified.
- Don't hard-wrap comments. Only use linebreaks for new paragraphs. Let the editor soft wrap content.
- Prefer negative if statements in combination with early returns/continues. Rather than nesting multiple positive if statements, just check if a condition is False, then return/continue in a loop. This reduces indentation.
- This project uses Python 3.12. You can use `dict`, `list`, `tuple` instead of the imports from `typing`. You can use `| None` instead of `Optional`.
- Use single-backticks for variables. We use Markdown and [pdoc3](https://pdoc3.github.io/pdoc/) for docs rather than ReST and Sphinx.
- File descriptors from `open()` are called `fd`.
- Use types where possible, including `jaxtyping` hints.
- Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
- Variables referring to a absolute filepath should be suffixed with `_fpath`. Filenames are `_fname`. Directories are `_dpath`.
- Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
- Only use `setup` for naming functions that don't return anything.

- You can use `gh` to access issues and PRs on GitHub to gather more context. We use GitHub issues a lot to share ideas and communicate about problems, so you should almost always check to see if there's a relevant GitHub issue for whatever you're working on.
- Write single-line commit messages; never say you co-authored a commit.

# Tensor Variables

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).
The key for these suffixes:

- b: batch size
- d: model dimension

For example, an batched activation tensor with shape (batch, d_model) is `x_bd`.

>>>> CONTRIBUTING.md
# Contributing

## TL;DR

Install [uv](https://docs.astral.sh/uv/).
Clone this repository, then from the root directory:

```sh
uv run python  # TODO
```

You also need [yek](https://github.com/bodo-run/yek) for generating docs.

## Coding Style & Conventions

* Don't hard-wrap comments. Only use linebreaks for new paragraphs. Let the editor soft wrap content.
* Use single-backticks for variables. We use Markdown and [pdoc3](https://pdoc3.github.io/pdoc/) for docs rather than ReST and Sphinx.
* File descriptors from `open()` are called `fd`.
* Use types where possible, including `jaxtyping` hints.
* Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
* Variables referring to a filepath should be suffixed with `_fpath`. Directories are `_dpath`.
* Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
* Only use `setup` for naming functions that don't return anything.

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).

The key for these suffixes:

* b: batch size
* d: model dimension

For example, an batched activation tensor with shape (batch, d_model) is `x_bd`.

## Testing & Linting

`justfile` contains commands for testing and linting.

`just lint` will format and lint.
`just test` will format, lint and test, then report coverage.

## Commit / PR Checklist

1. Run `just test`.
2. Check that there are no regressions. Unless you are certain tests are not needed, the coverage % should either stay the same or increase.
3. Run `just docs`.
4. Fix any missing doc links.

>>>> README.md
# vcell

[Shared Google Doc](https://docs.google.com/document/d/1oHJQUAFk9mrhPrLqkglq_e3yufYDEOKCF3VvDT-MHpc/edit?usp=sharing)

This is the repo for a [virtual cell challenge](https://virtualcellchallenge.org) submission.

To install, clone this repository (via SSH is probably easier, but you do you).

In the project root directory, run `uv run experiments/00_mean_baseline.py`.
The first invocation should create a virtual environment.

## Getting Started

1. (If you have a Google Cloud project and approved TPU usage) Install `gcloud`. The [official instructions](https://cloud.google.com/sdk/docs/install) are shitty. [Here's what I did](src/vcell/install-gcloud.md).
2. [Make a TPU VM](src/vcell/make-a-tpu-vm.md).
3. Clone this repo onto your TPU VM.
4. [Sync the input data to the TPU VM](src/vcell/tpu-tricks.md).
5. Run training.
6. [Sync the outputs to your local machine](src/vcell/tpu-tricks.md).
7. Run `uv run scripts/submit_vcc.py --pred pred_raw.h5ad`.
8. Submit `pred_raw.prep.vcc` to the leaderboard.

## Setup

**Editor:**
I would add `docs/api/` to whatever `.ignore`-like system you have so that they don't show up.
We commit them to git so that they can be browsed on GitHub, but you don't want to edit them by hand because they are generated by [pdoc3](https://pdoc3.github.io/pdoc/).

**Dependencies:** [uv](https://docs.astral.sh/uv/) is the new dependency manager in Python.
There's a short blog post [here](https://samuelstevens.me/writing/uv) describing uv but it's well documented all over the internet.
We use it in this project for the easiest dependency management ever.
Please install it--it's very high quality software.

**Automated Tooling:** [just](https://github.com/casey/just) is a decent language-agnostic replacement for `package.json` scripts.
It supports dependencies and is pretty fast.
It can be installed with brew (`brew install just`) or from a binary because it doesn't really need to be updated frequently.

> I am happy to change to make or another language-agnostic system if we don't want to go with just.

If you run `just --list` in any folder in the repo, it will list all the recipes.
Running `just` without any args will run the first recipe and its dependencies.

When writing new recipes, assume that you are on a generic unix-like system and that the only non-unix tools are `uv` and `just` itself.
So don't use `fd`, `rg`, `npm`, etc.

**AI Tooling (Optional):**
My favorite is [aider](https://aider.chat/).
I won't put a whole tutorial here, but I use Sonnet-3.5 with $20 of API credits loaded up.
The [AGENTS.md](AGENTS.md) file is used by aider so that the coding assistant follows conventions by default.
I symlink CLAUDE.md to AGENTS.md so that Claude Code will read it.

## Data

You need to make an account on [https://virtualcellchallenge.org](https://virtualcellchallenge.org) and then you can download the data.
The data provided by the challenge is 15GB.

Data should be extracted to the root project folder. Final structure will look like: `/vcell/data/vcc_data`.

## Docs

API docs are generated from Python docstrings using [pdoc3](https://pdoc3.github.io/pdoc/).
You can run `just docs` to generate new API docs in `docs/api`.
To write good docstrings that can be parsed, try to follow the conventions in existing code.

The docs recipe also generates `docs/api/llms.txt` which is all of the API docs in one .txt file.
This is great for copy-pasting into an LLM to ask it how to do something with our codebase.

## Dependencies

I hate dependencies.
I haaaaate dependencies.
That being said, here are some great tools that I use in nearly every project (including this one).

[tyro](https://brentyi.github.io/tyro/) turns functions into Python scripts.
By adding docstrings and types to everything (good practice anyways!) you can get great command-line interfaces to your Python scripts.

[beartype](https://beartype.readthedocs.io/en/stable/) is an "open-source pure-Python PEP-compliant near-real-time hybrid runtime-static third-generation type-checker."
While the docs can be a little corny, it adds very fast runtime type checking to your functions, meaning you never need to write any code that checks for types.
It is a runtime alternative to MyPy for when you have some objects typed `any` that you still want to type-check.
Please decorate every function with it.

>>>> __init__.py
"""
.. include:: install-gcloud.md

.. include:: make-a-tpu-vm.md

.. include:: tpu-tricks.md
"""

>>>> current-issue.md
# Issue #31: Compute per-dataset highly-variable genes (HVGs)

## Issue Description
Compute highly-variable genes for each dataset using Seurat v3 method, saving 2000 HVGs per dataset to enable downstream global HVG basis construction.

## Technical Requirements
1. Ensure raw counts, not log-normalized data
2. Use Seurat v3 variant: `scanpy.pp.highly_variable_genes(flavor="seurat_v3", n_top_genes=2000, layer="counts", batch_key if present)`
3. Handle large file-backed datasets efficiently (e.g., KOLF dataset)
4. Leverage column-wise access for performance on large datasets
5. Save outputs:
   - `hvgs_2k.txt`: List of 2000 highly variable gene names
   - `hvg_scores.json`: Scores/statistics for each gene (means, variances, variances_norm)

## Context in Pipeline
This issue is part of a larger HVG-based modeling pipeline:
1. **Issue #31 (current)**: Compute HVGs per dataset → outputs per-dataset HVG lists and scores
2. **Issue #32**: Build global HVG basis from per-dataset HVGs → creates unified 2k gene list
3. **Issue #33**: Normalize datasets consistently (CP10K + log1p) 
4. **Issue #36**: Implement transformer for the HVGs
5. **Issue #34**: Multi-source DataLoader for training

The HVGs are critical for dimensionality reduction - focusing the model on the most informative genes across datasets.

## Current Implementation Status
✅ **Completed:**
- Implemented `highly_variable_genes_seurat_v3_rows()` and `highly_variable_genes_seurat_v3_cols()` in `src/vcell/pp/hvgs.py`
- Functions handle both row-major (CSR) and column-major (CSC) sparse matrices efficiently
- Memory-efficient streaming implementation for file-backed datasets
- ~94% overlap with scanpy's reference implementation (acceptable tradeoff for memory efficiency)
- Comprehensive test suite with 22 tests including property-based testing
- Edge case handling (single cell, empty matrices, NaN values, etc.)
- Variance stabilization using lowess fitting (simplified from scanpy's loess)

## Completion Criteria for Senior Review

To convince a senior co-worker this issue is truly complete, we need:

### 1. ✅ Core Algorithm Implementation
- [x] Seurat v3 HVG selection with variance stabilizing transformation
- [x] Row-major and column-major streaming implementations
- [x] ~94% accuracy vs scanpy (documented tradeoff)
- [x] Comprehensive test coverage

### 2. 🔄 Performance Optimization (In Progress)
- [ ] Minimize number of passes over data (currently 2 passes)
- [ ] Benchmark on actual large datasets (KOLF)
- [ ] Memory profiling to ensure file-backed efficiency
- [ ] Document performance characteristics

### 3. ⏳ Production Pipeline Integration
- [ ] Script to process all datasets in config files
- [ ] Output formatting:
  - [ ] `hvgs_2k.txt` with gene names (one per line)
  - [ ] `hvg_scores.json` with full statistics
- [ ] Error handling for malformed/missing data
- [ ] Logging and progress reporting
- [ ] Handle batch_key parameter if present in dataset

### 4. ⏳ Dataset Processing
- [ ] Process all scPerturb datasets (K562_essential, K562_gwps, HepG2, Jurkat)
- [ ] Process VCC training data
- [ ] Process KOLF dataset (large, column-optimized)
- [ ] Verify outputs are consistent across datasets

### 5. ⏳ Documentation & Validation
- [ ] Document expected runtime for each dataset
- [ ] Validate HVG selections make biological sense
- [ ] Compare with known HVGs from literature if available
- [ ] Document any dataset-specific considerations

## Remaining Work

1. **Immediate (Today):**
   - Create production script using our HVG functions to process datasets
   - Implement output saving (hvgs_2k.txt and hvg_scores.json)
   - Add batch_key support if needed

2. **Performance (Priority):**
   - Analyze current data access patterns
   - Reduce from 2 passes to 1 if possible
   - Benchmark on KOLF dataset

3. **Integration:**
   - Process all datasets from configs
   - Verify outputs feed correctly into Issue #32 (global HVG basis)

## Success Metrics
- All datasets processed without memory errors
- KOLF processing completes in reasonable time (<2 hours)
- Output files correctly formatted for downstream pipeline
- HVG selections are biologically sensible (housekeeping genes not overrepresented)
>>>> data/__init__.py
from .naive_dataloader import Config as PerturbationConfig
from .naive_dataloader import PerturbationDataloader

__all__ = ["PerturbationConfig", "PerturbationDataloader"]

>>>> data/harmonize.py
# src/vcell/data/harmonize.py
import collections
import dataclasses
import pathlib
import re

import anndata as ad
import beartype
import numpy as np
import scanpy as sc
from jaxtyping import Bool, Int, jaxtyped


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class GeneMap:
    """Mapping from a dataset's columns to the canonical VCC gene space."""

    n_genes: int
    """Number of VCC genes"""
    present_mask: Bool[np.ndarray, " G"]
    """which VCC genes exist in this dataset"""
    ds_cols: Int[np.ndarray, " K"]
    """dataset column indices to take"""
    vcc_cols: Int[np.ndarray, " K"]
    """destination VCC columns"""
    stats: dict[str, int]
    """counts for sanity reporting"""

    def lift_to_vcc(self, x_ds: Int[np.ndarray, "..."]) -> Int[np.ndarray, "..."]:
        """
        Project dataset matrix slice (n, n_vars_ds) into VCC order (n, G), filling missing with zeros.
        """
        out = np.zeros((x_ds.shape[0], self.n_genes), dtype=np.float32)
        out[:, self.vcc_cols] = x_ds[:, self.ds_cols]
        return out


@beartype.beartype
class GeneVocab:
    """
    Canonical VCC gene space built from the VCC .h5ad.
    - Prefers Ensembl IDs (stable).
    - Keeps symbols for unique-only fallback.
    """

    def __init__(self, vcc_h5ad: str | pathlib.Path):
        vcc = sc.read(str(vcc_h5ad), backed="r")
        if "gene_id" not in vcc.var.columns:
            raise ValueError(
                "Expected VCC .var to contain a 'gene_id' column (Ensembl)."
            )

        self.n_genes = vcc.n_vars

        self.vcc_ens: list[str] = [strip_ens_version(s) for s in vcc.var["gene_id"]]
        self.vcc_sym: list[str] = vcc.var.index.astype(str).tolist()

        # Ensembl -> VCC index (unique by construction)
        self._ens_to_idx: dict[str, int] = {e: i for i, e in enumerate(self.vcc_ens)}
        # Symbol -> list of indices (can be non-unique)
        self._sym_to_idxs: dict[str, list[int]] = collections.defaultdict(list)
        for i, s in enumerate(self.vcc_sym):
            self._sym_to_idxs[s].append(i)

    def make_map(self, ds: ad.AnnData, *, gene_id_col: str = "ensembl_id") -> GeneMap:
        """
        Create a GeneMap from a dataset.
        """

        ds_sym = list(ds.var_names)
        ds_ens = [strip_ens_version(s) for s in ds.var[gene_id_col].tolist()]

        assert len(ds_sym) == len(ds_ens)

        present_mask = np.zeros(self.n_genes, dtype=bool)
        ds_cols: list[int] = []
        vcc_cols: list[int] = []

        n_ens_match = 0
        n_sym_match = 0
        n_sym_ambig = 0

        for j, (ens, sym) in enumerate(zip(ds_ens, ds_sym)):
            if ens and ens in self._ens_to_idx:
                i = self._ens_to_idx[ens]
                ds_cols.append(j)
                vcc_cols.append(i)
                present_mask[i] = True
                n_ens_match += 1
            else:
                cand = self._sym_to_idxs.get(sym, [])
                if len(cand) == 1:
                    i = cand[0]
                    ds_cols.append(j)
                    vcc_cols.append(i)
                    present_mask[i] = True
                    n_sym_match += 1
                elif len(cand) > 1:
                    n_sym_ambig += 1
                    # skip ambiguous symbols

        ds_cols = np.asarray(ds_cols, dtype=int)
        vcc_cols = np.asarray(vcc_cols, dtype=int)
        stats = dict(
            vcc_genes=self.n_genes,
            ds_vars=len(ds_sym),
            matched_by_ensembl=int(n_ens_match),
            matched_by_symbol=int(n_sym_match),
            skipped_ambiguous_symbol=int(n_sym_ambig),
            total_matched=int(len(ds_cols)),
            coverage=int(present_mask.sum()),
        )
        return GeneMap(
            n_genes=self.n_genes,
            present_mask=present_mask,
            ds_cols=ds_cols,
            vcc_cols=vcc_cols,
            stats=stats,
        )


@beartype.beartype
def strip_ens_version(s: str) -> str:
    """ENSG00000187634.5 -> ENSG00000187634"""
    return re.sub(r"\.\d+$", "", s)

>>>> data/naive_dataloader.py
# src/vcell/data/naive_dataloader.py
import collections
import dataclasses
import difflib
import logging
import pathlib
import warnings

import beartype
import numpy as np
import scanpy as sc


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    h5ad_fpath: pathlib.Path
    """Path to the h5ad file containing perturbation data."""

    set_size: int = 256
    """Number of cells to include in each example set."""

    genes: list[str] = dataclasses.field(default_factory=list)
    """List of gene names to select from the dataset. If empty, all genes are used."""

    pert_col: str = "target_gene"
    """Column name in the AnnData object that contains perturbation information."""

    cell_line_col: str = "cell_line"
    """Column name in the AnnData object that contains cell line information."""

    control_code: str = "non-targeting"
    """Value that identifies control cells in the perturbation column."""

    seed: int = 0
    """Random seed."""


@beartype.beartype
class PerturbationDataloader:
    """
    At each step, randomly samples one unique combination of cell line and perturbation. If there are no observations for the combination, it samples another.
    """

    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.logger = logging.getLogger(self.__class__.__name__)
        self.rng = np.random.default_rng(seed=cfg.seed)

        # Read the h5ad file
        self.adata = sc.read(self.cfg.h5ad_fpath, backed="r")

        # Apply genes if provided
        if self.cfg.genes:
            # Try to find matching genes in the data
            available_genes = self.adata.var_names.tolist()
            selected_indices = []
            for gene in self.cfg.genes:
                if gene in available_genes:
                    selected_indices.append(available_genes.index(gene))
            if selected_indices:
                self.adata = self.adata[:, selected_indices]

        # Check if data needs normalization
        self._ensure_normalized()

        # Validate column names exist
        self._validate_column(self.cfg.pert_col)
        self._validate_column(self.cfg.cell_line_col)

        # Create ID mappings first
        unique_perts = self.adata.obs[self.cfg.pert_col].unique()
        self.pert2id = {pert: i for i, pert in enumerate(sorted(unique_perts))}

        unique_clines = self.adata.obs[self.cfg.cell_line_col].unique()
        self.cline2id = {cline: i for i, cline in enumerate(sorted(unique_clines))}

    def __iter__(self) -> collections.abc.Iterator[dict]:
        pool = []
        # Randomly sample from the pool
        while True:
            if not pool:
                # If pool is empty, regenerate it
                pool = list(
                    self.adata.obs.groupby(
                        [self.cfg.cell_line_col, self.cfg.pert_col], observed=False
                    ).groups.keys()
                )

            # Randomly select a combination
            idx = self.rng.integers(0, len(pool))
            cell_line, pert = pool.pop(idx)

            # Get perturbed cells for this combination
            pert_mask = (self.adata.obs[self.cfg.cell_line_col] == cell_line) & (
                self.adata.obs[self.cfg.pert_col] == pert
            )
            pert_indices = np.where(pert_mask)[0]

            # Skip if not enough perturbed cells
            if len(pert_indices) < self.cfg.set_size:
                self.logger.debug(
                    "Skipping (%s, %s) with only %d indices.",
                    cell_line,
                    pert,
                    len(pert_indices),
                )
                continue

            # Just match cell line for controls
            control_mask = (self.adata.obs[self.cfg.cell_line_col] == cell_line) & (
                self.adata.obs[self.cfg.pert_col] == self.cfg.control_code
            )
            control_indices = np.where(control_mask)[0]

            # Skip if no control cells available
            if len(control_indices) == 0:
                self.logger.debug(
                    "Skipping (%s, %s) (no control cells).", cell_line, pert
                )
                continue

            # Sample perturbed cells
            sampled_pert_indices = self.rng.choice(
                pert_indices, size=self.cfg.set_size, replace=False
            )

            # Sample control cells (with replacement if needed)
            sampled_control_indices = self.rng.choice(
                control_indices, size=self.cfg.set_size, replace=True
            )

            # Get the expression data
            control_data = self._get_cell_data(sampled_control_indices.tolist())
            target_data = self._get_cell_data(sampled_pert_indices.tolist())

            yield {
                "control": control_data,
                "target": target_data,
                "pert": self.pert2id[pert],
                "cell_line": self.cline2id[cell_line],
            }

    def _ensure_normalized(self):
        """Ensure data is depth-normalized and log1p transformed."""
        # For backed mode, we assume data is already normalized
        # This is because we can't modify backed data
        # In production, this should be checked/documented

        self.logger.warning("Data normalization check not implemented for backed mode")
        warnings.warn(
            "Normalization check not implemented for backed AnnData objects",
            UserWarning,
        )

    def _validate_column(self, col_name: str) -> None:
        """Validate that a column exists, suggesting alternatives if not."""
        if col_name not in self.adata.obs.columns:
            available_cols = list(self.adata.obs.columns)
            # Find similar column names
            suggestions = difflib.get_close_matches(
                col_name, available_cols, n=5, cutoff=0.4
            )

            # Combine suggestions
            all_suggestions = list(dict.fromkeys(suggestions))[:5]

            error_msg = f"Column '{col_name}' not found in AnnData.obs."
            if all_suggestions:
                error_msg += f" Did you mean one of these? {all_suggestions}"
            error_msg += f" Available columns: {available_cols[:10]}{'...' if len(available_cols) > 10 else ''}"

            raise KeyError(error_msg)

    def _get_cell_data(self, indices: list[int]) -> np.ndarray:
        """Get expression data for given cell indices."""
        # h5py requires indices to be sorted and unique
        # Create mapping to handle duplicates
        unique_indices = sorted(set(indices))

        # Get data for unique indices
        data = self.adata[unique_indices].X
        if hasattr(data, "toarray"):
            data = data.toarray()

        # Create lookup for unique indices
        idx_to_data = {idx: data[i] for i, idx in enumerate(unique_indices)}

        # Build result with proper duplicates
        result = np.stack([idx_to_data[idx] for idx in indices])

        return result.astype(np.float32)

>>>> helpers.py
# src/vcell/helpers.py
import collections.abc
import dataclasses
import logging
import pathlib
import subprocess
import time
import typing as tp

import beartype


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress", total: int = 0):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
            total: If non-zero, how long the iterable is.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)
        self.total = total

    def __iter__(self):
        start = time.time()

        try:
            total = len(self)
        except TypeError:
            total = None

        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if total is not None:
                    pred_min = (total - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        total,
                        (i + 1) / total * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        if self.total > 0:
            return self.total

        # Will throw exception.
        return len(self.it)


@beartype.beartype
class batched_idx:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """

    def __init__(self, total_size: int, batch_size: int):
        """
        Args:
            total_size: total number of examples
            batch_size: maximum distance between the generated indices
        """
        self.total_size = total_size
        self.batch_size = batch_size

    def __iter__(self) -> collections.abc.Iterator[tuple[int, int]]:
        """Yield (start, end) index pairs for batching."""
        for start in range(0, self.total_size, self.batch_size):
            stop = min(start + self.batch_size, self.total_size)
            yield start, stop

    def __len__(self) -> int:
        """Return the number of batches."""
        return (self.total_size + self.batch_size - 1) // self.batch_size


@beartype.beartype
def current_git_commit() -> str | None:
    """
    Best-effort short SHA of the repo containing *this* file.

    Returns `None` when
    * `git` executable is missing,
    * we’re not inside a git repo (e.g. installed wheel),
    * or any git call errors out.
    """
    try:
        # Walk up until we either hit a .git dir or the FS root
        here = pathlib.Path(__file__).resolve()
        for parent in (here, *here.parents):
            if (parent / ".git").exists():
                break
        else:  # no .git found
            return None

        result = subprocess.run(
            ["git", "-C", str(parent), "rev-parse", "--short", "HEAD"],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            check=True,
        )
        return result.stdout.strip() or None
    except (FileNotFoundError, subprocess.CalledProcessError):
        return None


@beartype.beartype
def dict_to_dataclass(data: dict, cls: type) -> tp.Any:
    """Recursively convert a dictionary to a dataclass instance."""
    if not dataclasses.is_dataclass(cls):
        return data

    field_types = {f.name: f.type for f in dataclasses.fields(cls)}
    kwargs = {}

    for field_name, field_type in field_types.items():
        if field_name not in data:
            continue

        value = data[field_name]

        # Handle Optional types
        origin = tp.get_origin(field_type)
        args = tp.get_args(field_type)

        # Handle tuple[str, ...]
        if origin is tuple and args:
            kwargs[field_name] = tuple(value) if isinstance(value, list) else value
        # Handle list[DataclassType]
        elif origin is list and args and dataclasses.is_dataclass(args[0]):
            kwargs[field_name] = [dict_to_dataclass(item, args[0]) for item in value]
        # Handle regular dataclass fields
        elif dataclasses.is_dataclass(field_type):
            kwargs[field_name] = dict_to_dataclass(value, field_type)
        # Handle pathlib.Path
        elif field_type is pathlib.Path or (
            origin is tp.Union and pathlib.Path in args
        ):
            kwargs[field_name] = pathlib.Path(value) if value is not None else value
        else:
            kwargs[field_name] = value

    return cls(**kwargs)


@beartype.beartype
def get_non_default_values(obj: tp.Any, default_obj: tp.Any) -> dict:
    """Recursively find fields that differ from defaults."""
    obj_dict = dataclasses.asdict(obj)
    default_dict = dataclasses.asdict(default_obj)

    diff = {}
    for key, value in obj_dict.items():
        default_value = default_dict.get(key)
        if value != default_value:
            diff[key] = value

    return diff


@beartype.beartype
def merge_configs(base: tp.Any, overrides: dict) -> tp.Any:
    """Recursively merge override values into a base config."""
    if not overrides:
        return base

    base_dict = dataclasses.asdict(base)

    for key, value in overrides.items():
        if key in base_dict:
            # For nested dataclasses, merge recursively
            if isinstance(value, dict) and dataclasses.is_dataclass(getattr(base, key)):
                base_dict[key] = dataclasses.asdict(
                    merge_configs(getattr(base, key), value)
                )
            else:
                base_dict[key] = value

    return dict_to_dataclass(base_dict, type(base))


def check_grain_ops(ops: list[object]):
    import cloudpickle

    for op in ops:
        try:
            cloudpickle.dumps(op)
        except TypeError as err:
            raise AssertionError(f"Failed to pickle {op}: {err}")

>>>> install-gcloud.md
## Install `gcloud`

A simple guide to installing the `gcloud` binary so that it can be easily uninstalled.

1. Download and extract the package.
2. Move it somewhere useful.
3. Add the directory to your $PATH.
4. Login to Google Cloud.

Download the right package from https://cloud.google.com/sdk/docs/install for your computer.

<details>
<summary>My Choice</summary>

I clicked the macOS tab and then chose the macOS 64-bit Apple Silicon option: https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz

You can download and extract this to whatever directory you want. We will move it.

</details>

```
$ pwd
/Users/samstevens/Development/vcell
$ wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz
--2025-08-15 09:51:22--  https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-darwin-arm.tar.gz
Resolving dl.google.com (dl.google.com)... 74.125.21.91, 74.125.21.190, 74.125.21.136, ...
Connecting to dl.google.com (dl.google.com)|74.125.21.91|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 56538937 (54M) [application/gzip]
Saving to: ‘google-cloud-cli-darwin-arm.tar.gz’

google-cloud-cli-darwin-arm.tar.gz              100%[====================================================================================================>]  53.92M  31.4MB/s    in 1.7s

2025-08-15 09:51:24 (31.4 MB/s) - ‘google-cloud-cli-darwin-arm.tar.gz’ saved [56538937/56538937]
$ ls
google-cloud-cli-darwin-arm.tar.gz
$ tar -xzf google-cloud-cli-darwin-arm.tar.gz  # -x extract -z use gzip -f filepath
$ ls
google-cloud-cli-darwin-arm.tar.gz  google-cloud-sdk/
```

I store all these crappy non-pip tools in `~/.local/pkg`. You can put it wherever you want because we will eventually add it to our `$PATH`.

```
$ mv google-cloud-sdk ~/.local/pkg
$ ls ~/.local/pkg
aws-cli/  google-cloud-sdk/
```

Then I add the `bin/` to my path.
I use fish, so it's just `fish_add_path ~/.local/pkg/google-cloud-sdk/bin`.
If you use a different shell, then do whatever you do to add `~/.local/pkg/google-cloud-sdk/bin` to your path.

Then you can run:

```
$ gcloud version
Google Cloud SDK 533.0.0
bq 2.1.22
core 2025.08.01
gcloud-crc32c 1.0.0
gsutil 5.35
Updates are available for some Google Cloud CLI components.  To install them,
please run:
  $ gcloud components update
```

Now you have gcloud installed in a way that is easy to delete!
You need to run `glcoud init` and (maybe) `gcloud auth login` and login with your google account.

>>>> make-a-tpu-vm.md
## Make a TPU VM

TL;DR:

```sh
gcloud compute tpus tpu-vm create $NAME \
  --project $PROJECT_ID \
  --zone $ZONE \
  --accelerator-type $TPU_VERSION \
  --version $IMAGE
```

* `$NAME` can be whatever you want. You will reference it when you want to connect to the TPU VM.
* `$PROJECT_ID` is the project ID you shared with the TPU Research Cloud program folks. It's in the email you get from them (see below).
* The `$TPU_VERSION` and `$ZONE` variables depend on what you get access to. That's in your email.
* The `$IMAGE` variable depends on the `$TPU_VERSION`, and you have to look it up here: https://cloud.google.com/tpu/docs/runtimes. v4 and older use tpu-ubuntu2204-base.

![Image of my email from TRC, showing that I have access to `v2-8` TPU VMs in `us-central1-f`](/docs/assets/tpu-email.jpg)

You will need to run this command multiple times until it succeeds.

Once you have a TPU VM, you can see them all with:

```
$ gcloud compute tpus tpu-vm list --project $PROJECT_ID --zone $ZONE
NAME   ZONE           ACCELERATOR_TYPE  TYPE  TOPOLOGY  NETWORK  RANGE          STATUS
tpu-2  us-central1-f  v2-8              V2    2x2       default  10.128.0.0/20  READY
```

Then you can connect via SSH:

```sh
gcloud compute tpus tpu-vm ssh $NAME \
  --project $PROJECT_ID \
  --zone $ZONE
```

You should have sudo access.

>>>> metrics.py
# src/vcell/metrics.py
import typing as tp

import beartype
import equinox as eqx
import jax.numpy as jnp
from jaxtyping import Array, Float, jaxtyped

__all__ = ["DEDetails", "RunningMean", "compute_mae", "compute_pds", "compute_de"]


@jaxtyped(typechecker=beartype.beartype)
class DEDetails(eqx.Module):
    """Immutable result packet for DE agreement metrics."""

    overlap: float
    """e.g., Jaccard/F1 over true-significant genes."""
    pr_auc: float
    """PR-AUC of predicted DE vs true DE."""
    spearman_r: float
    """rank corr of (signed) logFC."""
    n_true_sig: int
    n_pred_sig: int


@jaxtyped(typechecker=beartype.beartype)
class RunningMean(eqx.Module):
    """Numerically stable running mean of scalar values.

    Note: prefer returning a *new* instance rather than in-place mutation
    to keep things functional/JAX-friendly.
    """

    total: Float[Array, ""]
    count: Float[Array, ""]

    @staticmethod
    @jaxtyped(typechecker=beartype.beartype)
    def zero() -> "RunningMean":
        return RunningMean(total=jnp.array(0.0), count=jnp.array(0.0))

    @jaxtyped(typechecker=beartype.beartype)
    def update(
        self, value: Float[Array, ""], weight: Float[Array, ""] | int = 1
    ) -> "RunningMean":
        total = self.total + value * jnp.asarray(weight, dtype=self.total.dtype)
        count = self.count + jnp.asarray(weight, dtype=self.count.dtype)
        return RunningMean(total=total, count=count)

    @jaxtyped(typechecker=beartype.beartype)
    def merge(self, other: "RunningMean") -> "RunningMean":
        return RunningMean(
            total=self.total + other.total, count=self.count + other.count
        )

    @jaxtyped(typechecker=beartype.beartype)
    def compute(self) -> Float[Array, ""]:
        # Returns NaN if count==0; caller should guard or accept NaN to signal "empty".
        return self.total / self.count


@jaxtyped(typechecker=beartype.beartype)
def compute_mae(
    pred: Float[Array, "... g"],
    true: Float[Array, "... g"],
    *,
    mask: Float[Array, "... g"] | None = None,
) -> Float[Array, "..."]:
    """Per-example MAE across genes. Reduces over the last (gene) axis only.

    Shapes:
      pred, true: [..., g]
      mask (optional): same shape; 1.0 keeps a gene, 0.0 drops it.
    Returns:
      mae: [...]  (one scalar per leading example/perturbation index)
    """
    diff = jnp.abs(pred - true)
    if mask is not None:
        # Avoid divide-by-zero: normalise by sum(mask) along gene axis.
        masked = diff * mask
        denom = jnp.clip(jnp.sum(mask, axis=-1), a_min=1e-12)
        return jnp.sum(masked, axis=-1) / denom
    return jnp.mean(diff, axis=-1)


@jaxtyped(typechecker=beartype.beartype)
def compute_pds(
    pred: Float[Array, "p g"],
    true: Float[Array, "p g"],
    *,
    topk: tuple[int, ...] = (1, 5, 10),
) -> dict[str, Float[Array, ""]]:
    """Perturbation Discrimination Score.

    Intent:
      For each perturbation i, rank true profiles by distance to pred[i].
      Report mean inverse rank and top-k accuracy.

    Returns:
        {
            "mean_inv_rank": float,
            "top1": float,
            "top5": float,
            ...
        }
    """
    p, g = pred.shape
    if p == 0:
        raise ValueError("p (number of perturbations) must be > 0.")

    # Pairwise L1 distances, smaller is better.
    # [p, p, g] -> [p, p]
    dist = jnp.abs(pred[:, None, :] - true[None, :, :]).sum(-1)

    # Regular ranking logic
    order = jnp.argsort(dist, axis=1, stable=True)
    true_idx = jnp.arange(p)
    # Rank = first position (1-based) where the correct index appears
    pos = jnp.argmax(order == true_idx[:, None], axis=1) + 1
    mrr = jnp.mean(1.0 / pos.astype(jnp.float32))

    # Top-k metrics (dedup + clamp)
    uniq_topk = tuple(sorted(set(topk)))
    out = {"mean_inv_rank": mrr}
    for k in uniq_topk:
        kk = int(max(1, min(k, p)))
        top = jnp.mean((pos <= kk).astype(jnp.float32))
        out[f"top{kk}"] = top
    return out


@jaxtyped(typechecker=beartype.beartype)
def compute_de(
    pred_pert: Float[Array, "r g"],
    pred_ctrl: Float[Array, "r g"],
    true_pert: Float[Array, "r g"],
    true_ctrl: Float[Array, "r g"],
    *,
    fdr: float = 0.05,
    test: tp.Literal["wilcoxon"] = "wilcoxon",
    two_sided: bool = True,
) -> DEDetails:
    """Differential-expression agreement (skeleton).

    Intent:
      1) Make DE calls (pred vs ctrl) and (true vs ctrl) with the same test+FDR.
      2) Compare sets/ranks: overlap (e.g., Jaccard/F1), PR-AUC, Spearman on |logFC| or signed logFC.

    Notes:
      - Callers should pass consistent normalisation/gene order.
      - r = number of 'replicates' per condition (can be 1 if using pseudobulks).

    Returns:
      DEDetails(overlap=..., pr_auc=..., spearman_r=..., n_true_sig=..., n_pred_sig=...)
    """
    raise NotImplementedError(
        "compute_de is a skeleton; implement DE test, BH-FDR, and comparisons."
    )

>>>> nn/loss.py
import beartype
import jax.numpy as jnp
from jaxtyping import Array, Float, jaxtyped


@jaxtyped(typechecker=beartype.beartype)
def _pairwise_lp(X, Y, p: float = 1.0, eps: float = 1e-12):
    """Return pairwise ||x_i - y_j||^p. Shapes: X [S,G], Y [S,G] -> [S,S]."""
    # squared euclidean
    x2 = (X * X).sum(-1, keepdims=True)  # [S,1]
    y2 = (Y * Y).sum(-1, keepdims=True).T  # [1,S]
    d2 = jnp.clip(x2 + y2 - 2.0 * (X @ Y.T), 0.0, jnp.inf)  # numerical safety
    if p == 2.0:
        return d2  # already squared L2
    # general p: ||.||^p = (||.||_2^2)^(p/2)
    return jnp.power(d2 + eps, 0.5 * p)


@jaxtyped(typechecker=beartype.beartype)
def mmd2_energy_kernel(
    pred: Float[Array, "set genes"], target: Float[Array, "set genes"], p: float = 1.0
) -> Float[Array, ""]:
    """
    The squared MMD between the predicted and observed cell sets is computed in Eq. (19) of the STATE paper.
    """
    s, g = pred.shape
    d_pp = _pairwise_lp(pred, pred, p)
    d_tt = _pairwise_lp(target, target, p)
    d_pt = _pairwise_lp(pred, target, p)
    # k = -dist^p
    k_pp = -d_pp
    k_tt = -d_tt
    k_pt = -d_pt
    mmd2 = (k_pp.sum() + k_tt.sum() - 2.0 * k_pt.sum()) / (s * s)
    return mmd2

>>>> nn/optim.py
import dataclasses
import typing as tp

import beartype
import optax


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    alg: tp.Literal["sgd", "adam", "adamw"] = "adam"
    """Optimizer algorithm."""
    learning_rate: float = 3e-4
    """Learning rate."""
    grad_clip: float = 1.0
    """Maximum gradient norm."""


@beartype.beartype
def make(cfg: Config):
    if cfg.alg == "sgd":
        optim = optax.sgd(learning_rate=cfg.learning_rate)
    elif cfg.alg == "adam":
        optim = optax.adam(learning_rate=cfg.learning_rate)
    elif cfg.alg == "adamw":
        optim = optax.adamw(learning_rate=cfg.learning_rate)
    else:
        tp.assert_never(cfg.alg)

    if cfg.grad_clip > 0:
        optim = optax.chain(optax.clip_by_global_norm(cfg.grad_clip), optim)

    return optim

>>>> pp/__init__.py
"""
Preprocessing functions for single-cell data.
"""

from .hvgs import (
    highly_variable_genes_seurat_v3_cols,
    highly_variable_genes_seurat_v3_rows,
)

__all__ = [
    "highly_variable_genes_seurat_v3_cols",
    "highly_variable_genes_seurat_v3_rows",
]

>>>> pp/hvgs.py
"""
Highly variable gene selection methods.
"""

import anndata as ad
import beartype
import numpy as np
import pandas as pd
import scipy.interpolate
import statsmodels.nonparametric.smoothers_lowess

from .. import helpers


def _compute_seurat_v3_variances(
    means_raw: np.ndarray,
    variances_cp10k: np.ndarray,
    n_cells: int,
    span: float = 0.3,
    eps: float = 1e-12,
) -> tuple[np.ndarray, np.ndarray]:
    """
    Compute Seurat v3 standardized variances using lowess fitting.

    This implements the variance stabilizing transformation (VST) from Seurat v3.
    Instead of using loess like scanpy, we use lowess which is simpler but similar.

    Returns:
        variances: Standardized variances (residuals from the trend)
        variances_norm: Normalized variances used for ranking
    """
    # Filter to genes with non-zero variance to avoid log(0)
    mask = variances_cp10k > 0
    if not mask.any() or mask.sum() < 3:
        # All genes have zero variance or too few genes for fitting
        # Return simple standardized variances without trend fitting
        return variances_cp10k, np.zeros_like(means_raw)

    # Work in log space
    log_means = np.log10(means_raw[mask] + eps)
    log_vars = np.log10(variances_cp10k[mask] + eps)

    # Sort for lowess fitting
    order = np.argsort(log_means)
    x_sorted = log_means[order]
    y_sorted = log_vars[order]

    # Fit lowess with robustifying iterations
    try:
        smooth = statsmodels.nonparametric.smoothers_lowess.lowess(
            endog=y_sorted,
            exog=x_sorted,
            frac=span,
            it=3,  # Robust iterations to downweight outliers
            return_sorted=True,
        )
        xs_smooth, ys_smooth = smooth[:, 0], smooth[:, 1]

        # Remove near-duplicate x values for interpolation
        keep = np.concatenate(([True], np.diff(xs_smooth) > 1e-12))
        xs_smooth, ys_smooth = xs_smooth[keep], ys_smooth[keep]

        # Check if we have enough points after deduplication
        if len(xs_smooth) < 2:
            # Not enough unique points for interpolation
            return variances_cp10k, np.zeros_like(means_raw)
    except (ValueError, np.linalg.LinAlgError):
        # Lowess fitting failed, return simple variances
        return variances_cp10k, np.zeros_like(means_raw)

    # Create interpolator for prediction
    interpolator = scipy.interpolate.PchipInterpolator(
        xs_smooth, ys_smooth, extrapolate=True
    )

    # Predict expected log variance for all genes
    expected_log_vars = interpolator(log_means)

    # Compute standardized variances (in original scale)
    # This is simplified from scanpy's full implementation
    # Scanpy uses clipping and additional normalization
    std_factor = 10**expected_log_vars
    std_factor = np.sqrt(std_factor)

    # Compute standardized variance
    # This is a simplified version - the actual Seurat v3 does more complex clipping
    variances_standardized = np.zeros_like(means_raw)
    variances_standardized[mask] = variances_cp10k[mask] / (std_factor**2)

    # For normalized variances used in ranking, compute residuals in log space
    variances_norm = np.zeros_like(means_raw)
    variances_norm[mask] = log_vars - expected_log_vars

    return variances_standardized, variances_norm


@beartype.beartype
def highly_variable_genes_seurat_v3_rows(
    adata: ad.AnnData,
    *,
    n_top_genes: int | None = None,
    layer: str | None = None,
    batch_size: int = 2048,
    target_sum: float = 10_000,
    span: float = 0.3,
) -> pd.DataFrame:
    """
    Compute Seurat v3 HVGs by streaming rows (cells). Efficient for CSR/row-major data.

    Returns DataFrame with 'means', 'variances', 'variances_norm', 'highly_variable'.
    """
    n_cells, n_genes = adata.shape

    # Get data matrix
    X = adata.layers[layer] if layer else adata.X

    # Initialize accumulators
    weighted_g = np.zeros((n_genes,), dtype=np.float64)
    squared_g = np.zeros((n_genes,), dtype=np.float64)
    det_g = np.zeros((n_genes,), dtype=np.int64)
    means_raw_g = np.zeros(n_genes, dtype=np.float64)

    # Also need raw squared sums for raw variance
    raw_squared_g = np.zeros(n_genes, dtype=np.float64)

    # Stream through rows (cells)
    for start, end in helpers.progress(
        helpers.batched_idx(n_cells, batch_size), desc="rows"
    ):
        x_bg = X[start:end]
        if hasattr(x_bg, "toarray"):
            x_bg = x_bg.toarray()
        sum_b = x_bg.sum(axis=1)
        weight_b = target_sum / np.maximum(sum_b, 1.0)
        weighted_bg = weight_b[:, None] * x_bg
        weighted_g += weighted_bg.sum(axis=0)
        squared_g += (weighted_bg * weighted_bg).sum(axis=0)
        det_g += (x_bg > 0).sum(axis=0)
        means_raw_g += x_bg.sum(axis=0)
        raw_squared_g += (x_bg * x_bg).sum(axis=0)  # Raw squared values
    means_raw_g /= n_cells

    # Compute CP10K variance
    means_norm = weighted_g / n_cells
    variances_cp10k = (squared_g / n_cells) - (means_norm**2)
    variances_cp10k = np.maximum(variances_cp10k, 0)  # Numerical safety
    # Convert to unbiased estimator (if we have more than 1 cell)
    if n_cells > 1:
        variances_cp10k = variances_cp10k * n_cells / (n_cells - 1)

    # Compute Seurat v3 standardized variances
    variances, variances_norm = _compute_seurat_v3_variances(
        means_raw_g, variances_cp10k, n_cells, span
    )

    # Select top genes
    highly_variable = np.zeros(n_genes, dtype=bool)
    if n_top_genes is not None:
        top_indices = np.argsort(variances_norm)[-n_top_genes:]
        highly_variable[top_indices] = True

    # Create DataFrame
    # Note: For plotting we need raw variances, not CP10K normalized
    # Compute raw variances from the raw data
    variances_raw = (raw_squared_g / n_cells) - (means_raw_g**2)
    variances_raw = np.maximum(variances_raw, 0)
    if n_cells > 1:
        variances_raw = variances_raw * n_cells / (n_cells - 1)

    result = pd.DataFrame(
        {
            "means": means_raw_g,
            "variances": variances_raw,  # Raw variances for plotting
            "variances_norm": variances_norm,
            "highly_variable": highly_variable,
        },
        index=adata.var_names,
    )

    return result


@beartype.beartype
def highly_variable_genes_seurat_v3_cols(
    adata: ad.AnnData,
    *,
    n_top_genes: int | None = None,
    layer: str | None = None,
    batch_size: int = 2048,
    target_sum: float = 10_000,
    span: float = 0.3,
) -> pd.DataFrame:
    """
    Compute Seurat v3 HVGs by streaming columns (genes). Efficient for CSC/col-major data.
    Makes two passes: first to compute library sizes, then to compute statistics.

    Returns DataFrame with 'means', 'variances', 'variances_norm', 'highly_variable'.
    """
    n_cells, n_genes = adata.shape

    # Get data matrix
    X = adata.layers[layer] if layer else adata.X

    # Initialize accumulators
    weighted_g = np.zeros((n_genes,), dtype=np.float64)
    squared_g = np.zeros((n_genes,), dtype=np.float64)
    det_g = np.zeros((n_genes,), dtype=np.int64)

    # First pass: compute library sizes
    sizes_n = np.zeros(n_cells, dtype=np.float64)
    for start, end in helpers.batched_idx(n_genes, batch_size):
        sizes_n += np.asarray(X[:, start:end].sum(axis=1)).squeeze()

    sizes_n = np.maximum(sizes_n, 1.0)  # guard against empty cells
    weights_n = (target_sum / sizes_n).astype(np.float64)

    # Second pass: compute weighted statistics and raw means
    means_raw = np.zeros(n_genes, dtype=np.float64)
    raw_squared_g = np.zeros(n_genes, dtype=np.float64)
    for start, end in helpers.batched_idx(n_genes, batch_size):
        x_ng = X[:, start:end].copy()

        # Handle both sparse and dense
        if hasattr(x_ng, "toarray"):
            # Sparse matrix
            # Compute raw means and raw squared sums before normalization
            raw_sum = np.asarray(x_ng.sum(axis=0)).squeeze()
            means_raw[start:end] = raw_sum / n_cells
            # Raw squared sum (before normalization)
            x_ng_raw = X[:, start:end].copy()
            x_ng_raw.data **= 2
            raw_squared_g[start:end] = np.asarray(x_ng_raw.sum(axis=0)).squeeze()
            # Now apply normalization for variance computation
            x_ng.data *= weights_n[x_ng.indices]
            weighted_g[start:end] += np.asarray(x_ng.sum(axis=0)).squeeze()
            x_ng.data **= 2
            squared_g[start:end] += np.asarray(x_ng.sum(axis=0)).squeeze()
            # Detection counts (only works for CSC format)
            if hasattr(X[:, start:end], "indptr"):
                det_g[start:end] += np.diff(X[:, start:end].indptr)
            else:
                det_g[start:end] += (X[:, start:end] > 0).sum(axis=0)
        else:
            # Dense matrix
            means_raw[start:end] = x_ng.sum(axis=0) / n_cells
            raw_squared_g[start:end] = (x_ng**2).sum(axis=0)
            x_ng_weighted = x_ng * weights_n[:, None]
            weighted_g[start:end] += x_ng_weighted.sum(axis=0)
            squared_g[start:end] += (x_ng_weighted**2).sum(axis=0)
            det_g[start:end] += (x_ng > 0).sum(axis=0)

    # Compute CP10K variance
    means_norm = weighted_g / n_cells
    variances_cp10k = (squared_g / n_cells) - (means_norm**2)
    variances_cp10k = np.maximum(variances_cp10k, 0)  # Numerical safety
    # Convert to unbiased estimator (if we have more than 1 cell)
    if n_cells > 1:
        variances_cp10k = variances_cp10k * n_cells / (n_cells - 1)

    # Compute Seurat v3 standardized variances
    variances, variances_norm = _compute_seurat_v3_variances(
        means_raw, variances_cp10k, n_cells, span
    )

    # Select top genes
    highly_variable = np.zeros(n_genes, dtype=bool)
    if n_top_genes is not None:
        top_indices = np.argsort(variances_norm)[-n_top_genes:]
        highly_variable[top_indices] = True

    # Create DataFrame
    # Compute raw variances for plotting
    variances_raw = (raw_squared_g / n_cells) - (means_raw**2)
    variances_raw = np.maximum(variances_raw, 0)
    if n_cells > 1:
        variances_raw = variances_raw * n_cells / (n_cells - 1)

    result = pd.DataFrame(
        {
            "means": means_raw,
            "variances": variances_raw,  # Raw variances for plotting
            "variances_norm": variances_norm,
            "highly_variable": highly_variable,
        },
        index=adata.var_names,
    )

    return result

>>>> scratch.py
import logging

import mudata as md

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)

logging.info("Finished imports.")
mdata = md.read_h5mu(
    "/Volumes/samuel-stevens-2TB/datasets/KOLF_Pan_Genome_Aggregate.h5mu",
    backed="r",
)
logging.info("Read file.")
rna = mdata["rna"].X[:10]
logging.info("Got RNA in memory.")
breakpoint()

>>>> tpu-tricks.md
## TPU Tricks

`gcloud` can `scp` files and folders to and from TPU VMs.

For instance, to get the vcc.zip file onto your TPU VM:

```sh
gcloud compute tpus tpu-vm scp $VCC_ZIP $NAME:$PROJECT_ROOT/data/inputs \
  --project $PROJECT_ID \
  --zone $ZONE
```

For me, from my project root, I run:

```sh
gcloud compute tpus tpu-vm scp data/inputs/vcc.zip tpu-2:projects/vcell/data/inputs --project trc-project-466816 --zone us-central1-f
```

Then on my TPU VM, I just run `unzip vcc.zip`.

Or to get the predictions file onto my laptop so that I can submit it:

```sh
gcloud compute tpus tpu-vm scp $NAME:$PROJECT_ROOT/pred_raw.h5ad . \
  --project $PROJECT_ID \
  --zone $ZONE
```

For me, from my project root, I run:

```sh
gcloud compute tpus tpu-vm scp tpu-2:~/projects/vcell/pred_raw.h5ad . --project trc-project-466816 --zone us-central1-f
```

Note the `.` to signify where on your laptop it should go; `.` means this directory.

## Adding Persistent Disks

gcloud compute disks create tpu-data --size 200GB --type pd-standard --zone us-central1-f --project trc-project-466816

gcloud alpha compute tpus tpu-vm attach-disk tpu-2 --zone us-central1-f --disk tpu-data --project trc-project-466816 --mode read-write


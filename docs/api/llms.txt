>>>> AGENTS.md
See /CONTRIBUTING.md for guidelines on codestyle.

>>>> CLAUDE.md
See /CONTRIBUTING.md for guidelines on codestyle.

>>>> CONTRIBUTING.md
# Contributing

## TL;DR

Install [uv](https://docs.astral.sh/uv/).
Clone this repository, then from the root directory:

```sh
uv run python  # TODO
```

You also need [yek](https://github.com/bodo-run/yek) for generating docs.

## Coding Style & Conventions

* Don't hard-wrap comments. Only use linebreaks for new paragraphs. Let the editor soft wrap content.
* Use single-backticks for variables. We use Markdown and [pdoc3](https://pdoc3.github.io/pdoc/) for docs rather than ReST and Sphinx.
* File descriptors from `open()` are called `fd`.
* Use types where possible, including `jaxtyping` hints.
* Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
* Variables referring to a filepath should be suffixed with `_fpath`. Directories are `_dpath`.
* Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
* Only use `setup` for naming functions that don't return anything.

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).

The key for these suffixes:

* b: batch size
* d: model dimension

For example, an batched activation tensor with shape (batch, d_model) is `x_bd`.

## Testing & Linting

`justfile` contains commands for testing and linting.

`just lint` will format and lint.
`just test` will format, lint and test, then report coverage.

## Commit / PR Checklist

1. Run `just test`.
2. Check that there are no regressions. Unless you are certain tests are not needed, the coverage % should either stay the same or increase.
3. Run `just docs`.
4. Fix any missing doc links.

>>>> README.md
# vcell

[Shared Google Doc](https://docs.google.com/document/d/1oHJQUAFk9mrhPrLqkglq_e3yufYDEOKCF3VvDT-MHpc/edit?usp=sharing)

This is the repo for a [virtual cell challenge](https://virtualcellchallenge.org) submission.

To install, clone this repository (via SSH is probably easier, but you do you).

In the project root directory, run `uv run experiments/00_mean_baseline.py`.
The first invocation should create a virtual environment.

## Setup

**Editor:**
I would add `docs/api/` to whatever `.ignore`-like system you have so that they don't show up.
We commit them to git so that they can be browsed on GitHub, but you don't want to edit them by hand because they are generated by [pdoc3](https://pdoc3.github.io/pdoc/).

**Dependencies:** [uv](https://docs.astral.sh/uv/) is the new dependency manager in Python.
There's a short blog post [here](https://samuelstevens.me/writing/uv) describing uv but it's well documented all over the internet.
We use it in this project for the easiest dependency management ever.
Please install it--it's very high quality software.

**Automated Tooling:** [just](https://github.com/casey/just) is a decent language-agnostic replacement for `package.json` scripts.
It supports dependencies and is pretty fast.
It can be installed with brew (`brew install just`) or from a binary because it doesn't really need to be updated frequently.

> I am happy to change to make or another language-agnostic system if we don't want to go with just.

If you run `just --list` in any folder in the repo, it will list all the recipes.
Running `just` without any args will run the first recipe and its dependencies.

When writing new recipes, assume that you are on a generic unix-like system and that the only non-unix tools are `uv` and `just` itself.
So don't use `fd`, `rg`, `npm`, etc.

**AI Tooling (Optional):**
My favorite is [aider](https://aider.chat/).
I won't put a whole tutorial here, but I use Sonnet-3.5 with $20 of API credits loaded up.
The [AGENTS.md](AGENTS.md) file is used by aider so that the coding assistant follows conventions by default.
I symlink CLAUDE.md to AGENTS.md so that Claude Code will read it.

## Data

You need to make an account on [https://virtualcellchallenge.org](https://virtualcellchallenge.org) and then you can download the data.
The data provided by the challenge is 15GB.

Data should be extracted to the root project folder. Final structure will look like: `/vcell/data/vcc_data`.
## Baseline



## Docs

API docs are generated from Python docstrings using [pdoc3](https://pdoc3.github.io/pdoc/).
You can run `just docs` to generate new API docs in `docs/api`.
To write good docstrings that can be parsed, try to follow the conventions in existing code.

The docs recipe also generates `docs/api/llms.txt` which is all of the API docs in one .txt file.
This is great for copy-pasting into an LLM to ask it how to do something with our codebase.

## Dependencies

I hate dependencies.
I haaaaate dependencies.
That being said, here are some great tools that I use in nearly every project (including this one).

[tyro](https://brentyi.github.io/tyro/) turns functions into Python scripts.
By adding docstrings and types to everything (good practice anyways!) you can get great command-line interfaces to your Python scripts.

[beartype](https://beartype.readthedocs.io/en/stable/) is an "open-source pure-Python PEP-compliant near-real-time hybrid runtime-static third-generation type-checker."
While the docs can be a little corny, it adds very fast runtime type checking to your functions, meaning you never need to write any code that checks for types.
It is a runtime alternative to MyPy for when you have some objects typed `any` that you still want to type-check.
Please decorate every function with it.

>>>> __init__.py

>>>> data/__init__.py
from .naive_dataloader import Config as PerturbationConfig
from .naive_dataloader import PerturbationDataloader

__all__ = ["PerturbationConfig", "PerturbationDataloader"]

>>>> data/naive_dataloader.py
# src/vcell/data/naive_dataloader.py
import collections
import dataclasses
import difflib
import logging
import pathlib
import warnings

import beartype
import numpy as np
import scanpy as sc


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    h5ad_fpath: pathlib.Path
    """Path to the h5ad file containing perturbation data."""

    set_size: int = 256
    """Number of cells to include in each example set."""

    genes: list[str] = dataclasses.field(default_factory=list)
    """List of gene names to select from the dataset. If empty, all genes are used."""

    pert_col: str = "target_gene"
    """Column name in the AnnData object that contains perturbation information."""

    cell_line_col: str = "cell_line"
    """Column name in the AnnData object that contains cell line information."""

    control_code: str = "non-targeting"
    """Value that identifies control cells in the perturbation column."""

    seed: int = 0
    """Random seed."""


@beartype.beartype
class PerturbationDataloader:
    """
    At each step, randomly samples one unique combination of cell line and perturbation. If there are no observations for the combination, it samples another.
    """

    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.logger = logging.getLogger(self.__class__.__name__)
        self.rng = np.random.default_rng(seed=cfg.seed)

        # Read the h5ad file
        self.adata = sc.read(self.cfg.h5ad_fpath, backed="r")

        # Apply genes if provided
        if self.cfg.genes:
            # Try to find matching genes in the data
            available_genes = self.adata.var_names.tolist()
            selected_indices = []
            for gene in self.cfg.genes:
                if gene in available_genes:
                    selected_indices.append(available_genes.index(gene))
            if selected_indices:
                self.adata = self.adata[:, selected_indices]

        # Check if data needs normalization
        self._ensure_normalized()

        # Validate column names exist
        self._validate_column(self.cfg.pert_col)
        self._validate_column(self.cfg.cell_line_col)

        # Create ID mappings first
        unique_perts = self.adata.obs[self.cfg.pert_col].unique()
        self.pert2id = {pert: i for i, pert in enumerate(sorted(unique_perts))}

        unique_clines = self.adata.obs[self.cfg.cell_line_col].unique()
        self.cline2id = {cline: i for i, cline in enumerate(sorted(unique_clines))}

    def __iter__(self) -> collections.abc.Iterator[dict]:
        pool = []
        # Randomly sample from the pool
        while True:
            if not pool:
                # If pool is empty, regenerate it
                pool = list(
                    self.adata.obs.groupby(
                        [self.cfg.cell_line_col, self.cfg.pert_col], observed=False
                    ).groups.keys()
                )

            # Randomly select a combination
            idx = self.rng.integers(0, len(pool))
            cell_line, pert = pool.pop(idx)

            # Get perturbed cells for this combination
            pert_mask = (self.adata.obs[self.cfg.cell_line_col] == cell_line) & (
                self.adata.obs[self.cfg.pert_col] == pert
            )
            pert_indices = np.where(pert_mask)[0]

            # Skip if not enough perturbed cells
            if len(pert_indices) < self.cfg.set_size:
                self.logger.debug(
                    "Skipping (%s, %s) with only %d indices.",
                    cell_line,
                    pert,
                    len(pert_indices),
                )
                continue

            # Just match cell line for controls
            control_mask = (self.adata.obs[self.cfg.cell_line_col] == cell_line) & (
                self.adata.obs[self.cfg.pert_col] == self.cfg.control_code
            )
            control_indices = np.where(control_mask)[0]

            # Skip if no control cells available
            if len(control_indices) == 0:
                self.logger.debug(
                    "Skipping (%s, %s) (no control cells).", cell_line, pert
                )
                continue

            # Sample perturbed cells
            sampled_pert_indices = self.rng.choice(
                pert_indices, size=self.cfg.set_size, replace=False
            )

            # Sample control cells (with replacement if needed)
            sampled_control_indices = self.rng.choice(
                control_indices, size=self.cfg.set_size, replace=True
            )

            # Get the expression data
            control_data = self._get_cell_data(sampled_control_indices.tolist())
            target_data = self._get_cell_data(sampled_pert_indices.tolist())

            yield {
                "control": control_data,
                "target": target_data,
                "pert": self.pert2id[pert],
                "cell_line": self.cline2id[cell_line],
            }

    def _ensure_normalized(self):
        """Ensure data is depth-normalized and log1p transformed."""
        # For backed mode, we assume data is already normalized
        # This is because we can't modify backed data
        # In production, this should be checked/documented

        self.logger.warning("Data normalization check not implemented for backed mode")
        warnings.warn(
            "Normalization check not implemented for backed AnnData objects",
            UserWarning,
        )

    def _validate_column(self, col_name: str) -> None:
        """Validate that a column exists, suggesting alternatives if not."""
        if col_name not in self.adata.obs.columns:
            available_cols = list(self.adata.obs.columns)
            # Find similar column names
            suggestions = difflib.get_close_matches(
                col_name, available_cols, n=5, cutoff=0.4
            )

            # Combine suggestions
            all_suggestions = list(dict.fromkeys(suggestions))[:5]

            error_msg = f"Column '{col_name}' not found in AnnData.obs."
            if all_suggestions:
                error_msg += f" Did you mean one of these? {all_suggestions}"
            error_msg += f" Available columns: {available_cols[:10]}{'...' if len(available_cols) > 10 else ''}"

            raise KeyError(error_msg)

    def _get_cell_data(self, indices: list[int]) -> np.ndarray:
        """Get expression data for given cell indices."""
        # h5py requires indices to be sorted and unique
        # Create mapping to handle duplicates
        unique_indices = sorted(set(indices))

        # Get data for unique indices
        data = self.adata[unique_indices].X
        if hasattr(data, "toarray"):
            data = data.toarray()

        # Create lookup for unique indices
        idx_to_data = {idx: data[i] for i, idx in enumerate(unique_indices)}

        # Build result with proper duplicates
        result = np.stack([idx_to_data[idx] for idx in indices])

        return result.astype(np.float32)

>>>> helpers.py
# src/vcell/helpers.py
import collections.abc
import logging
import pathlib
import subprocess
import time

import beartype


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress", total: int = 0):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
            total: If non-zero, how long the iterable is.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)
        self.total = total

    def __iter__(self):
        start = time.time()

        try:
            total = len(self)
        except TypeError:
            total = None

        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if total is not None:
                    pred_min = (total - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        total,
                        (i + 1) / total * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        if self.total > 0:
            return self.total

        # Will throw exception.
        return len(self.it)


@beartype.beartype
class batched_idx:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """

    def __init__(self, total_size: int, batch_size: int):
        """
        Args:
            total_size: total number of examples
            batch_size: maximum distance between the generated indices
        """
        self.total_size = total_size
        self.batch_size = batch_size

    def __iter__(self) -> collections.abc.Iterator[tuple[int, int]]:
        """Yield (start, end) index pairs for batching."""
        for start in range(0, self.total_size, self.batch_size):
            stop = min(start + self.batch_size, self.total_size)
            yield start, stop

    def __len__(self) -> int:
        """Return the number of batches."""
        return (self.total_size + self.batch_size - 1) // self.batch_size


@beartype.beartype
def current_git_commit() -> str | None:
    """
    Best-effort short SHA of the repo containing *this* file.

    Returns `None` when
    * `git` executable is missing,
    * we’re not inside a git repo (e.g. installed wheel),
    * or any git call errors out.
    """
    try:
        # Walk up until we either hit a .git dir or the FS root
        here = pathlib.Path(__file__).resolve()
        for parent in (here, *here.parents):
            if (parent / ".git").exists():
                break
        else:  # no .git found
            return None

        result = subprocess.run(
            ["git", "-C", str(parent), "rev-parse", "--short", "HEAD"],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            check=True,
        )
        return result.stdout.strip() or None
    except (FileNotFoundError, subprocess.CalledProcessError):
        return None

>>>> metrics.py
# src/vcell/metrics.py
import typing as tp

import beartype
import equinox as eqx
import jax
import jax.numpy as jnp
from jaxtyping import Array, Float, jaxtyped

__all__ = [
    "DEDetails",
    "RunningMean",
    "compute_mae",
    "compute_pds",
    "compute_de",
    "pmean_scalar",
]


# -------------------------
# Result containers (eqx.Module)
# -------------------------


@jaxtyped(typechecker=beartype.beartype)
class DEDetails(eqx.Module):
    """Immutable-ish result packet for DE agreement metrics."""

    overlap: float  # e.g., Jaccard/F1 over true-significant genes
    pr_auc: float  # PR-AUC of predicted DE vs true DE
    spearman_r: float  # rank corr of (signed) logFC
    n_true_sig: int
    n_pred_sig: int


# -------------------------
# Tiny in-loop helper for streaming scalars
# -------------------------


@jaxtyped(typechecker=beartype.beartype)
class RunningMean(eqx.Module):
    """Numerically stable running mean of scalar values.

    Note: prefer returning a *new* instance rather than in-place mutation
    to keep things functional/JAX-friendly.
    """

    total: Float[Array, ""]
    count: Float[Array, ""]

    @staticmethod
    @jaxtyped(typechecker=beartype.beartype)
    def zero() -> "RunningMean":
        return RunningMean(total=jnp.array(0.0), count=jnp.array(0.0))

    @jaxtyped(typechecker=beartype.beartype)
    def update(
        self, value: Float[Array, ""], weight: Float[Array, ""] | int = 1
    ) -> "RunningMean":
        total = self.total + value * jnp.asarray(weight, dtype=self.total.dtype)
        count = self.count + jnp.asarray(weight, dtype=self.count.dtype)
        return RunningMean(total=total, count=count)

    @jaxtyped(typechecker=beartype.beartype)
    def merge(self, other: "RunningMean") -> "RunningMean":
        return RunningMean(
            total=self.total + other.total, count=self.count + other.count
        )

    @jaxtyped(typechecker=beartype.beartype)
    def compute(self) -> Float[Array, ""]:
        # Returns NaN if count==0; caller should guard or accept NaN to signal "empty".
        return self.total / self.count


# -------------------------
# Pure metric functions (can be called outside jit; gather first if distributed)
# -------------------------


@jaxtyped(typechecker=beartype.beartype)
def compute_mae(
    pred: Float[Array, "... g"],
    true: Float[Array, "... g"],
    *,
    mask: Float[Array, "... g"] | None = None,
) -> Float[Array, "..."]:
    """Per-example MAE across genes. Reduces over the last (gene) axis only.

    Shapes:
      pred, true: [..., g]
      mask (optional): same shape; 1.0 keeps a gene, 0.0 drops it.
    Returns:
      mae: [...]  (one scalar per leading example/perturbation index)
    """
    diff = jnp.abs(pred - true)
    if mask is not None:
        # Avoid divide-by-zero: normalise by sum(mask) along gene axis.
        masked = diff * mask
        denom = jnp.clip(jnp.sum(mask, axis=-1), a_min=1e-12)
        return jnp.sum(masked, axis=-1) / denom
    return jnp.mean(diff, axis=-1)


@jaxtyped(typechecker=beartype.beartype)
def compute_pds(
    pred_by_pert: Float[Array, "p g"],
    true_by_pert: Float[Array, "p g"],
    *,
    distance: tp.Literal["cosine", "euclidean"] = "cosine",
    topk: tuple[int, ...] = (1, 5, 10),
) -> dict[str, float]:
    """Perturbation Discrimination Score (skeleton).

    Intent:
      For each perturbation i, rank true profiles by distance to pred[i].
      Report mean inverse rank and top-k accuracy.

    Returns:
      {
        "mean_inv_rank": float,
        "top1": float,
        "top5": float,
        ...
      }
    """
    raise NotImplementedError(
        "compute_pds is a skeleton; implement ranking + reductions."
    )


@jaxtyped(typechecker=beartype.beartype)
def compute_de(
    pred_pert: Float[Array, "r g"],
    pred_ctrl: Float[Array, "r g"],
    true_pert: Float[Array, "r g"],
    true_ctrl: Float[Array, "r g"],
    *,
    fdr: float = 0.05,
    test: tp.Literal["wilcoxon"] = "wilcoxon",
    two_sided: bool = True,
) -> DEDetails:
    """Differential-expression agreement (skeleton).

    Intent:
      1) Make DE calls (pred vs ctrl) and (true vs ctrl) with the same test+FDR.
      2) Compare sets/ranks: overlap (e.g., Jaccard/F1), PR-AUC, Spearman on |logFC| or signed logFC.

    Notes:
      - Callers should pass consistent normalisation/gene order.
      - r = number of 'replicates' per condition (can be 1 if using pseudobulks).

    Returns:
      DEDetails(overlap=..., pr_auc=..., spearman_r=..., n_true_sig=..., n_pred_sig=...)
    """
    raise NotImplementedError(
        "compute_de is a skeleton; implement DE test, BH-FDR, and comparisons."
    )


# -------------------------
# Distributed convenience
# -------------------------


@jaxtyped(typechecker=beartype.beartype)
def pmean_scalar(x: Float[Array, ""], *, axis_name: str = "data") -> Float[Array, ""]:
    """Average a scalar across devices (for pmap) via lax.pmean.

    Usage:
      with jax.pmap(..., axis_name="data"):
          local = batch_mae.mean()
          global_mean = pmean_scalar(local, axis_name="data")
    """
    return jax.lax.pmean(x, axis_name)


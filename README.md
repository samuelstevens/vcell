# vcell

[Shared Google Doc](https://docs.google.com/document/d/1oHJQUAFk9mrhPrLqkglq_e3yufYDEOKCF3VvDT-MHpc/edit?usp=sharing)

This is the repo for a [virtual cell challenge](https://virtualcellchallenge.org) submission.

To install, clone this repository (via SSH is probably easier, but you do you).

In the project root directory, run `uv run experiments/00_mean_baseline.py`.
The first invocation should create a virtual environment.



## Setup

**Editor:**
I would add `docs/api/` to whatever `.ignore`-like system you have so that they don't show up.
We commit them to git so that they can be browsed on GitHub, but you don't want to edit them by hand because they are generated by [pdoc3](https://pdoc3.github.io/pdoc/).

**Dependencies:** [uv](https://docs.astral.sh/uv/) is the new dependency manager in Python.
There's a short blog post [here](https://samuelstevens.me/writing/uv) describing uv but it's well documented all over the internet.
We use it in this project for the easiest dependency management ever.
Please install it--it's very high quality software.

**Automated Tooling:** [just](https://github.com/casey/just) is a decent language-agnostic replacement for `package.json` scripts.
It supports dependencies and is pretty fast.
It can be installed with brew (`brew install just`) or from a binary because it doesn't really need to be updated frequently.

> I am happy to change to make or another language-agnostic system if we don't want to go with just.

If you run `just --list` in any folder in the repo, it will list all the recipes.
Running `just` without any args will run the first recipe and its dependencies.

When writing new recipes, assume that you are on a generic unix-like system and that the only non-unix tools are `uv` and `just` itself.
So don't use `fd`, `rg`, `npm`, etc.

**AI Tooling (Optional):**
My favorite is [aider](https://aider.chat/).
I won't put a whole tutorial here, but I use Sonnet-3.5 with $20 of API credits loaded up.
The [AGENTS.md](AGENTS.md) file is used by aider so that the coding assistant follows conventions by default.
I symlink CLAUDE.md to AGENTS.md so that Claude Code will read it.

## Data

You need to make an account on [https://virtualcellchallenge.org](https://virtualcellchallenge.org) and then you can download the data.
The data provided by the challenge is 15GB.

Data should be extracted to the root project folder. Final structure will look like: `/vcell/data/vcc_data`.

## Docs

API docs are generated from Python docstrings using [pdoc3](https://pdoc3.github.io/pdoc/).
You can run `just docs` to generate new API docs in `docs/api`.
To write good docstrings that can be parsed, try to follow the conventions in existing code.

The docs recipe also generates `docs/api/llms.txt` which is all of the API docs in one .txt file.
This is great for copy-pasting into an LLM to ask it how to do something with our codebase.

## Dependencies

I hate dependencies.
I haaaaate dependencies.
That being said, here are some great tools that I use in nearly every project (including this one).

[tyro](https://brentyi.github.io/tyro/) turns functions into Python scripts.
By adding docstrings and types to everything (good practice anyways!) you can get great command-line interfaces to your Python scripts.

[beartype](https://beartype.readthedocs.io/en/stable/) is an "open-source pure-Python PEP-compliant near-real-time hybrid runtime-static third-generation type-checker."
While the docs can be a little corny, it adds very fast runtime type checking to your functions, meaning you never need to write any code that checks for types.
It is a runtime alternative to MyPy for when you have some objects typed `any` that you still want to type-check.
Please decorate every function with it.

## Getting Started

1. Install `gcloud`.
2. Download VCC and scPerturb datasets.
3. Upload data to GCS ($$).
4. Start an experiment on a TPU VM.
5. Download the predictions from GCS ($$).
6. Prepare the predictions for submission.
7. Submit to the leaderboard.

**Install `gcloud`** (if you have a Google Cloud project and approved TPU usage).
The [official instructions](https://cloud.google.com/sdk/docs/install) are shitty.
[Here's what I did](src/vcell/install-gcloud.md).

**Download VCC and scPerturb datasets.**
You need to download the VCC data from the official challenge website.
Then use the download_scperturb.py script to download scPerturb data.

First do a dry-run:

```
[I] samstevens@localhoster ~/D/vcell (main)> uv run scripts/download_scperturb.py --out data/inputs/scperturb --records 13350497 --include Replogle Nadig --dry-run
Found 54 files across 1 record(s): 13350497
Selected 5 to download.

  - 13350497/NadigOConner2024_hepg2.h5ad  (811.2 MB)
  - 13350497/NadigOConner2024_jurkat.h5ad  (1233.7 MB)
  - 13350497/ReplogleWeissman2022_K562_essential.h5ad  (1475.1 MB)
  - 13350497/ReplogleWeissman2022_K562_gwps.h5ad  (8397.5 MB)
  - 13350497/ReplogleWeissman2022_rpe1.h5ad  (1179.6 MB)
```

Then run without `--dry-run` to actually download it: `uv run scripts/download_scperturb.py --out data/inputs/scperturb --records 13350497 --include Replogle Nadig`.


**Upload the data to GCS ($$).**

Follow the instructions [here](src/vcell/tpu-init.md#prerequisites).
Note that both uploading costs money (ingress fees) and storage costs money (storage fees).
Both are pretty cheap (on the order of tens of cents).

**Start an experiment on a TPU VM.**

```sh
gcloud compute tpus tpu-vm create TPUNAME1 \
  --zone us-central1-a \
  --accelerator-type v5litepod-16 \
  --version v2-alpha-tpuv5-lite \
  --spot \
  --metadata git-ref=main,gcs-bucket=gs://sam-vcc-us-central1/bucket,exp-script=experiments/14_repro_st_rn.py,exp-args='--cfg configs/14-repro-st-rn.toml --vcc-root $DATA_ROOT',wandb-api-key=11b55b27cf1dab08762cd33c62e329ed291aa5ae,wandb-project=vcell,wandb-entity=samuelstevens \
  --metadata-from-file startup-script=scripts/tpu-init.sh`
```

**Download the predictions from GCS ($$).**

TODO: Experiment scripts don't upload the predictions to GCS. They should. Once they do, then you can download it from GCS to your laptop. There will be egress fees (fees for moving data from a Google Cloud zone to outside that zone).

**Prepare the predictions for submission.**

If you downloaded a `pred_RUNID.h5ad` file from GCS, then run:

```sh
uv run scripts/submit_vcc.py --pred pred_RUNID.h5ad
```

This will produce a `pred_RUNID.prep.vcc` file.

**Submit to the leaderboard.**

Submit `pred_RUNID.prep.vcc` to the leaderboard.

